{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":13780822,"sourceType":"datasetVersion","datasetId":8744658}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ==================== OPTIMIZATION FLAGS ====================\nimport os\nos.environ['TF_XLA_FLAGS'] = '--tf_xla_auto_jit=2'\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\nos.environ['TF_GPU_THREAD_MODE'] = 'gpu_private'\nos.environ['TF_GPU_THREAD_COUNT'] = '2'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T17:34:42.955268Z","iopub.execute_input":"2025-11-18T17:34:42.955754Z","iopub.status.idle":"2025-11-18T17:34:42.964827Z","shell.execute_reply.started":"2025-11-18T17:34:42.955729Z","shell.execute_reply":"2025-11-18T17:34:42.964068Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"%%writefile env.py\n\n\"\"\"\nWaffle Game Environment - optimized for speed\n\"\"\"\nimport numpy as np\nimport json\nfrom typing import Tuple, List, Optional, Sequence\n\nclass Waffle(object):\n    def __init__(self, puzzles_file: str = \"waffles_shuffled.jsonl\",\n                 embeddings_file: str = \"letter_embeddings.npy\") -> None:\n        self.puzzles_file = puzzles_file\n        self.puzzles = self._load_puzzles()\n        self.embeddings = self._load_embeddings(embeddings_file)\n        self.grid_size = 21\n\n        self.row_words = [\n            [0, 1, 2, 3, 4],\n            [8, 9, 10, 11, 12],\n            [16, 17, 18, 19, 20]\n        ]\n        self.col_words = [\n            [0, 5, 8, 13, 16],\n            [2, 6, 10, 14, 18],\n            [4, 7, 12, 15, 20]\n        ]\n\n        # Pre-allocate arrays\n        self.current_state = np.zeros(21, dtype=np.int32)\n        self.target_state = np.zeros(21, dtype=np.int32)\n\n        # features: [green, yellow, emb(8)] => shape (21,10)\n        self.prev_features = np.zeros((21, 10), dtype=np.float32)\n        self.features_buffer = np.zeros((21, 10), dtype=np.float32)\n\n        # Precompute position->word mapping as numpy arrays for faster access\n        self.pos_to_word_positions = [None] * 21\n        for i in range(21):\n            wp = None\n            for word in self.row_words:\n                if i in word:\n                    wp = word\n                    break\n            if wp is None:\n                for word in self.col_words:\n                    if i in word:\n                        wp = word\n                        break\n            self.pos_to_word_positions[i] = np.array(wp, dtype=np.int32) if wp else None\n\n        # will be filled in reset\n        self.reset(0)\n\n    def _load_puzzles(self) -> List[dict]:\n        puzzles = []\n        with open(self.puzzles_file, \"r\") as f:\n            for line in f:\n                puzzles.append(json.loads(line))\n        return puzzles\n\n    def _load_embeddings(self, embeddings_file: str) -> np.ndarray:\n        try:\n            embeddings = np.load(embeddings_file)\n            return embeddings.astype(np.float32)\n        except FileNotFoundError:\n            return np.random.randn(27, 8).astype(np.float32)\n\n    def _string_to_array(self, flat21: str, out: np.ndarray) -> None:\n        # faster vectorized conversion\n        arr = np.frombuffer(flat21.encode('ascii'), dtype=np.uint8) - ord('a') + 1\n        out[:] = arr.astype(np.int32)\n\n    def _calculate_features_full(self, state: np.ndarray, out: np.ndarray) -> None:\n        \"\"\"Compute features for all positions (vectorized).\"\"\"\n        # green mask\n        green = (state == self.target_state).astype(np.float32)\n        out[:, 0] = green\n\n        # yellow: for each pos, check if current_letter matches any target letter in its word (excluding green)\n        for pos in range(21):\n            wp = self.pos_to_word_positions[pos]\n            if wp is None:\n                out[pos, 1] = 0.0\n                continue\n            current_letter = state[pos]\n            # check if current_letter equals target_state at any wp position (and not already green at pos)\n            matches = (self.target_state[wp] == current_letter)\n            out[pos, 1] = 1.0 if matches.any() and not green[pos] else 0.0\n\n        # embeddings\n        # index clipping\n        indices = np.clip(state.astype(np.int32), 0, 26)\n        out[:, 2:10] = self.embeddings[indices]\n\n    def _calculate_features_positions(self, state: np.ndarray, out: np.ndarray, positions: Sequence[int]) -> None:\n        \"\"\"Only update provided positions in 'out' using same logic as full.\"\"\"\n        for pos in positions:\n            current_letter = state[pos]\n            out[pos, :] = 0.0\n            # green\n            if current_letter == self.target_state[pos]:\n                out[pos, 0] = 1.0\n            else:\n                wp = self.pos_to_word_positions[pos]\n                if wp is not None:\n                    if np.any(self.target_state[wp] == current_letter):\n                        out[pos, 1] = 1.0\n            # embeddings\n            idx = int(current_letter) if current_letter <= 26 else 0\n            out[pos, 2:10] = self.embeddings[idx]\n\n    def _count_correct_positions(self, state: np.ndarray) -> int:\n        return int(np.sum(state == self.target_state))\n\n    def reset(self, index: int) -> None:\n        self.current_puzzle = self.puzzles[index]\n        self._string_to_array(self.current_puzzle[\"target_flat21\"], self.target_state)\n        self._string_to_array(self.current_puzzle[\"shuffled_flat21\"], self.current_state)\n        self.fixed_indices = set(self.current_puzzle[\"fixed_indices\"])\n        self.moves = 0\n        self.max_moves = 200\n        self.prev_correct_count = self._count_correct_positions(self.current_state)\n        # Precompute movable positions & valid actions once per puzzle\n        self.movable_positions = np.array([i for i in range(21) if i not in self.fixed_indices], dtype=np.int32)\n        # precompute valid actions (pos1,pos2) encoded as pos1*21+pos2 for pos1 < pos2\n        valid_actions = []\n        mp = self.movable_positions\n        for i in range(len(mp)):\n            for j in range(i+1, len(mp)):\n                valid_actions.append(int(mp[i] * 21 + mp[j]))\n        self._valid_actions_cache = np.array(valid_actions, dtype=np.int32)\n        # full features at start\n        self._calculate_features_full(self.current_state, self.prev_features)\n        self.features_buffer[:] = self.prev_features\n\n    def _update_state(self, action: int) -> None:\n        pos1 = action // 21\n        pos2 = action % 21\n        if pos1 not in self.fixed_indices and pos2 not in self.fixed_indices:\n            self.current_state[pos1], self.current_state[pos2] = self.current_state[pos2], self.current_state[pos1]\n        self.moves += 1\n\n    def _get_reward(self, action: int) -> float:\n        pos1 = action // 21\n        pos2 = action % 21\n        reward = -0.1\n\n        if np.array_equal(self.current_state, self.target_state):\n            return 1000.0 + reward\n\n        current_correct = self._count_correct_positions(self.current_state)\n        correct_change = current_correct - self.prev_correct_count\n\n        # Only recompute features for the two positions that changed (big speed win).\n        self._calculate_features_positions(self.current_state, self.features_buffer, [pos1, pos2])\n\n        if self.current_state[pos1] == self.current_state[pos2]:\n            reward += -1\n\n        if (self.current_state[pos2] == self.target_state[pos1] or\n            self.current_state[pos1] == self.target_state[pos2]):\n            reward += -10\n\n        if correct_change > 0:\n            reward += 1 * int(correct_change)\n        elif correct_change < 0:\n            reward += -2 * abs(int(correct_change))\n\n        # feature-change bonuses (only check pos1,pos2)\n        for pos in (pos1, pos2):\n            prev_green = self.prev_features[pos, 0]\n            prev_yellow = self.prev_features[pos, 1]\n            curr_green = self.features_buffer[pos, 0]\n            curr_yellow = self.features_buffer[pos, 1]\n\n            if prev_green == 0 and prev_yellow == 0 and curr_yellow == 1.0:\n                reward += 1\n            elif prev_yellow == 1.0 and curr_green == 0 and curr_yellow == 0:\n                reward += -1\n\n        self.prev_correct_count = current_correct\n        # copy updated positions back into prev_features (only the changed positions)\n        self.prev_features[[pos1, pos2], :] = self.features_buffer[[pos1, pos2], :]\n        return float(reward)\n\n    def _is_over(self) -> bool:\n        return np.array_equal(self.current_state, self.target_state) or self.moves >= self.max_moves\n\n    def _is_win(self) -> bool:\n        return np.array_equal(self.current_state, self.target_state)\n\n    def observe(self) -> np.ndarray:\n        # returns a view shaped for model\n        return self.prev_features.reshape(1, 21, 10)\n\n    def act(self, action: int) -> Tuple[np.ndarray, float, bool, bool]:\n        self._update_state(action)\n        reward = self._get_reward(action)\n        game_over = self._is_over()\n        game_win = self._is_win()\n        return self.observe(), reward, game_over, game_win\n\n    def get_valid_actions(self) -> List[int]:\n        # return cached list (fast)\n        return self._valid_actions_cache.tolist()\n\n    def get_num_actions(self) -> int:\n        return 21 * 21","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T17:34:43.066360Z","iopub.execute_input":"2025-11-18T17:34:43.066880Z","iopub.status.idle":"2025-11-18T17:34:43.075101Z","shell.execute_reply.started":"2025-11-18T17:34:43.066858Z","shell.execute_reply":"2025-11-18T17:34:43.074510Z"}},"outputs":[{"name":"stdout","text":"Writing env.py\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"%%writefile experience_replay.py\n\n# experience_replay.py (corrected)\n\"\"\"\nExperience Replay - circular buffer based for speed (corrected)\n\"\"\"\nimport numpy as np\nimport tensorflow as tf\nfrom typing import Tuple\n\nclass ExperienceReplay(object):\n    def __init__(self, max_memory: int = 5000, discount: float = 0.95) -> None:\n        self.max_memory = int(max_memory)\n        self.discount = float(discount)\n        # We'll store states flattened for compact storage; each state originally shape (1,21,10)\n        self.state_shape = (1, 21, 10)\n        flat_size = int(np.prod(self.state_shape))\n        self.states = np.zeros((self.max_memory, flat_size), dtype=np.float32)\n        self.next_states = np.zeros((self.max_memory, flat_size), dtype=np.float32)\n        self.actions = np.zeros((self.max_memory,), dtype=np.int32)\n        self.rewards = np.zeros((self.max_memory,), dtype=np.float32)\n        self.game_over = np.zeros((self.max_memory,), dtype=np.bool_)\n        self._size = 0\n        self._pos = 0\n\n    def add_experience(self, sars: list, game_over: bool) -> None:\n        # sars: [previous_state (1,21,10), action (int), reward (float), current_state (1,21,10)]\n        prev_s, action, reward, curr_s = sars\n        self.states[self._pos] = np.asarray(prev_s, dtype=np.float32).reshape(-1)\n        self.next_states[self._pos] = np.asarray(curr_s, dtype=np.float32).reshape(-1)\n        self.actions[self._pos] = int(action)\n        self.rewards[self._pos] = float(reward)\n        self.game_over[self._pos] = bool(game_over)\n        self._pos = (self._pos + 1) % self.max_memory\n        self._size = min(self._size + 1, self.max_memory)\n\n    def size(self) -> int:\n        \"\"\"Public accessor for number of stored transitions.\"\"\"\n        return int(self._size)\n\n    def get_qlearning_batch(self, model, batch_size: int = 32) -> Tuple[np.ndarray, np.ndarray]:\n        if self._size == 0:\n            raise ValueError(\"No experience to sample\")\n        num_samples = min(self._size, int(batch_size))\n        ids = np.random.choice(self._size, size=num_samples, replace=False)\n\n        # restore to (batch, 21, 10)\n        previous_states = self.states[ids].reshape((num_samples, 21, 10)).astype(np.float32)\n        current_states = self.next_states[ids].reshape((num_samples, 21, 10)).astype(np.float32)\n        action_ts = self.actions[ids].astype(np.int32)\n        rewards = self.rewards[ids].astype(np.float32)\n        game_over = self.game_over[ids]\n\n        # Ensure inputs are tf tensors with correct dtype/device\n        prev_tf = tf.constant(previous_states, dtype=tf.float32)\n        next_tf = tf.constant(current_states, dtype=tf.float32)\n\n        # Model inference (dtype stable)\n        targets = model(prev_tf, training=False).numpy()\n        Q_next = model(next_tf, training=False).numpy()\n        Q_sa = Q_next.max(axis=1)\n\n        # compute RHS then cast to targets' dtype\n        not_done = (~game_over).astype(np.float32)\n        rhs = (rewards + self.discount * Q_sa * not_done).astype(targets.dtype)\n\n        targets[np.arange(num_samples), action_ts] = rhs\n\n        return previous_states, targets","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T17:34:43.076255Z","iopub.execute_input":"2025-11-18T17:34:43.076451Z","iopub.status.idle":"2025-11-18T17:34:43.092863Z","shell.execute_reply.started":"2025-11-18T17:34:43.076437Z","shell.execute_reply":"2025-11-18T17:34:43.092209Z"}},"outputs":[{"name":"stdout","text":"Writing experience_replay.py\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# ==================== CELL 3: Main Training ====================\nimport tensorflow as tf\nprint(f\"TensorFlow: {tf.__version__}\")\nprint(f\"GPUs: {len(tf.config.list_physical_devices('GPU'))}\")\n\nimport json\nimport numpy as np\nimport logging\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Conv1D, Flatten, Dropout, BatchNormalization\nfrom tensorflow.keras.optimizers import Adam\n\nfrom experience_replay import ExperienceReplay\nfrom env import Waffle\n\n# ==================== GPU CONFIGURATION ====================\nprint(\"=\"*60)\nprint(\"GPU OPTIMIZATION\")\nprint(\"=\"*60)\n\ngpus = tf.config.list_physical_devices('GPU')\nif gpus:\n    try:\n        for gpu in gpus:\n            tf.config.experimental.set_memory_growth(gpu, True)\n        \n        # Critical optimizations\n        tf.config.threading.set_intra_op_parallelism_threads(8)\n        tf.config.threading.set_inter_op_parallelism_threads(4)\n        tf.config.optimizer.set_jit(True)\n        \n        print(f\"âœ“ {len(gpus)} GPU(s) configured\")\n        print(\"âœ“ XLA compilation enabled\")\n        print(\"âœ“ Thread optimization enabled\")\n    except RuntimeError as e:\n        print(f\"Warning: {e}\")\nelse:\n    print(\"âš ï¸ No GPU\")\nprint(\"=\"*60)\n\nimport tensorflow as tf\nfrom tensorflow.keras import mixed_precision\n\n# --- Enable mixed precision for speed (if GPU supports it) ---\npolicy = mixed_precision.Policy('mixed_float16')\nmixed_precision.set_global_policy(policy)\nprint(\"Mixed precision policy:\", mixed_precision.global_policy())\n\n# Keep XLA enabled (you already set jit via compile and optimizer options)\ntf.config.optimizer.set_jit(True)\n\n# Reduce Python prints in the hot loop (optional)\n# Comment out model.summary() if you want speed during startup\n\n# ==================== FIND DATASET ====================\ndataset_path = None\nembeddings_path = None\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        if filename == 'waffles_shuffled.jsonl':\n            dataset_path = os.path.join(dirname, filename)\n        if filename == 'letter_embeddings.npy':\n            embeddings_path = os.path.join(dirname, filename)\n\nif not dataset_path:\n    raise FileNotFoundError(\"waffles_shuffled.jsonl not found\")\n\nprint(f\"\\nâœ“ Dataset: {dataset_path}\")\nprint(f\"âœ“ Embeddings: {embeddings_path}\\n\")\n\n# ==================== MODEL DEFINITION ====================\ndef define_model(input_shape, hidden_size, num_actions, learning_rate=0.001, conv_filters=64):\n    model = Sequential([\n        Conv1D(conv_filters, 3, activation='relu', padding='same', input_shape=input_shape),\n        BatchNormalization(),\n        Conv1D(conv_filters, 3, activation='relu', padding='same'),\n        BatchNormalization(),\n        Conv1D(conv_filters * 2, 5, activation='relu', padding='same'),\n        BatchNormalization(),\n        Flatten(),\n        Dense(hidden_size, activation='relu'),\n        Dropout(0.2),\n        Dense(hidden_size // 2, activation='relu'),\n        Dropout(0.2),\n        Dense(num_actions)\n    ])\n    \n    from tensorflow.keras import mixed_precision\n    # Create base optimizer\n    base_opt = Adam(learning_rate=learning_rate)\n    # Wrap into LossScaleOptimizer if available (Keras mixed precision)\n    try:\n        opt = mixed_precision.LossScaleOptimizer(base_opt)\n    except Exception:\n        # fallback to base optimizer if wrapper not available\n        opt = base_opt\n\n    model.compile(\n        optimizer=opt,\n        loss='mse',\n        jit_compile=True\n    )\n\n    return model\n\n# ==================== TRAINING FUNCTION ====================\n\n@tf.function(reduce_retracing=True)\ndef train_step(model, inputs, targets):\n    with tf.GradientTape() as tape:\n        predictions = model(inputs, training=True)\n        # compute loss in float32 consistently\n        loss = tf.reduce_mean(tf.square(tf.cast(predictions, tf.float32) - tf.cast(targets, tf.float32)))\n\n    optimizer = model.optimizer\n\n    # If optimizer supports Keras LossScaleOptimizer API, use it; else fallback to manual scaling.\n    if hasattr(optimizer, \"get_scaled_loss\") and hasattr(optimizer, \"get_unscaled_gradients\"):\n        scaled_loss = optimizer.get_scaled_loss(loss)\n        scaled_grads = tape.gradient(scaled_loss, model.trainable_variables)\n        grads = optimizer.get_unscaled_gradients(scaled_grads)\n    else:\n        # manual loss-scaling fallback\n        loss_scale = tf.constant(1024.0, dtype=tf.float32)\n        scaled_loss = loss * loss_scale\n        scaled_grads = tape.gradient(scaled_loss, model.trainable_variables)\n        # unscale grads (guard against None)\n        grads = [g / loss_scale if g is not None else tf.zeros_like(v) for g, v in zip(scaled_grads, model.trainable_variables)]\n\n    # Replace any remaining None gradients with zeros\n    grads = [g if g is not None else tf.zeros_like(v) for g, v in zip(grads, model.trainable_variables)]\n\n    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n    return loss\n\ndef train_model(model, env, epochs, experience_replay, \n               epsilon_start, epsilon_end, epsilon_decay, batch_size):\n    \n    win_count = 0\n    total_rewards = []\n    epsilon = epsilon_start\n    num_actions = env.get_num_actions()\n    num_puzzles = len(env.puzzles)\n    \n    # Pre-compile prediction function\n    @tf.function\n    def predict_q(state):\n        return model(state, training=False)\n    \n    print(\"\\nğŸš€ Starting training...\\n\")\n    \n    for epoch in range(epochs):\n        epoch_loss = 0.0\n        batch_count = 0\n        \n        for i in range(num_puzzles):\n            env.reset(i)\n            current_state = env.observe()\n            game_over = False\n            episode_reward = 0\n            step_count = 0\n            \n            while not game_over:\n                previous_state = current_state\n                \n                # Action selection\n                if np.random.rand() <= epsilon:\n                    valid_actions = env.get_valid_actions()\n                    action = np.random.choice(valid_actions) if valid_actions else np.random.randint(0, num_actions)\n                else:\n                    q = predict_q(tf.constant(previous_state, dtype=tf.float32))\n                    action = int(tf.argmax(q[0], axis=-1).numpy())\n                \n                current_state, reward, game_over, game_win = env.act(action)\n                episode_reward += reward\n                step_count += 1\n                \n                if game_win:\n                    win_count += 1\n                \n                experience_replay.add_experience(\n                    [previous_state, int(action), reward, current_state], game_over\n                )\n                \n                # Batch training every 5 steps\n                if experience_replay.size() >= batch_size and step_count % 5 == 0:\n                    inputs, targets = experience_replay.get_qlearning_batch(model, batch_size)\n                    inputs_tensor = tf.convert_to_tensor(inputs, dtype=tf.float32)\n                    # ensure targets remain float32 for loss calculation (we cast preds to float32 inside train_step)\n                    targets_tensor = tf.convert_to_tensor(targets, dtype=tf.float32)\n                    loss = train_step(model, inputs_tensor, targets_tensor)\n                    epoch_loss += loss.numpy()\n                    batch_count += 1\n            \n            total_rewards.append(episode_reward)\n        \n        epsilon = max(epsilon_end, epsilon * epsilon_decay)\n        \n        # Progress reporting\n        if (epoch + 1) % 10 == 0 or epoch < 10:\n            avg_loss = epoch_loss / max(batch_count, 1)\n            avg_reward = np.mean(total_rewards[-100:]) if len(total_rewards) >= 100 else np.mean(total_rewards)\n            win_rate = win_count / ((epoch + 1) * num_puzzles) * 100\n            \n            print(f\"Ep {epoch+1:4d}/{epochs} | Loss: {avg_loss:8.4f} | Îµ: {epsilon:.4f} | \"\n                  f\"Wins: {win_count:4d} ({win_rate:5.1f}%) | AvgR: {avg_reward:7.2f}\")\n        \n        if (epoch + 1) % 50 == 0:\n            print(f\"\\n{'='*70}\")\n            print(f\"CHECKPOINT - Epoch {epoch+1}\")\n            print(f\"Win rate: {win_rate:.1f}% | Avg reward: {avg_reward:.2f}\")\n            print(f\"{'='*70}\\n\")\n    \n    final_win_rate = win_count / (epochs * num_puzzles) * 100\n    print(f\"\\nâœ… Training complete! Win rate: {final_win_rate:.1f}%\")\n    return model\n\nEPOCHS = 100\nEPSILON_START = 1.0\nEPSILON_END = 0.05\n# Prefer per-step linear decay; if per-epoch multiplicative, use:\nEPSILON_DECAY = 0.95\n\nMAX_MEMORY = 20000\nHIDDEN_SIZE = 256\nCONV_FILTERS = 64\nBATCH_SIZE = 128\nDISCOUNT = 0.99\nLEARNING_RATE = 3e-4\n\n# ==================== INITIALIZE ====================\nenv = Waffle(puzzles_file=dataset_path, embeddings_file=embeddings_path)\nnum_actions = env.get_num_actions()\ninput_shape = (21, 10)\n\nprint(f\"\\n{'='*70}\")\nprint(f\"TRAINING CONFIGURATION\")\nprint(f\"{'='*70}\")\nprint(f\"Puzzles:             {len(env.puzzles)}\")\nprint(f\"Actions:             {num_actions}\")\nprint(f\"Input shape:         {input_shape}\")\nprint(f\"Batch size:          {BATCH_SIZE}\")\nprint(f\"Hidden size:         {HIDDEN_SIZE}\")\nprint(f\"Epochs:              {EPOCHS}\")\nprint(f\"{'='*70}\\n\")\n\n# ==================== BUILD MODEL ====================\nmodel = define_model(\n    input_shape=input_shape,\n    hidden_size=HIDDEN_SIZE,\n    num_actions=num_actions,\n    learning_rate=LEARNING_RATE,\n    conv_filters=CONV_FILTERS\n)\n\nmodel.summary()\n\n# GPU warmup\nwarmup_input = tf.random.normal([1, 21, 10])\n_ = model(warmup_input, training=False)\nprint(f\"\\nâœ“ Model compiled with XLA\")\nprint(f\"âœ“ GPU warmup complete\\n\")\n\n# ==================== TRAIN ====================\nexp_replay = ExperienceReplay(max_memory=MAX_MEMORY, discount=DISCOUNT)\n\ntrained_model = train_model(\n    model=model,\n    env=env,\n    epochs=EPOCHS,\n    experience_replay=exp_replay,\n    epsilon_start=EPSILON_START,\n    epsilon_end=EPSILON_END,\n    epsilon_decay=EPSILON_DECAY,\n    batch_size=BATCH_SIZE\n)\n\n# ==================== SAVE ====================\noutput_dir = '/kaggle/working/model'\nos.makedirs(output_dir, exist_ok=True)\n\ntrained_model.save_weights(f'{output_dir}/model.weights.h5')\nwith open(f'{output_dir}/model.json', 'w') as f:\n    json.dump(trained_model.to_json(), f)\n\nprint(f\"\\nâœ… Model saved to {output_dir}/\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T17:34:43.113607Z","iopub.execute_input":"2025-11-18T17:34:43.114145Z","iopub.status.idle":"2025-11-18T20:01:05.911480Z","shell.execute_reply.started":"2025-11-18T17:34:43.114127Z","shell.execute_reply":"2025-11-18T20:01:05.910760Z"}},"outputs":[{"name":"stderr","text":"2025-11-18 17:34:44.542299: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1763487284.735563      48 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1763487284.791265      48 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"name":"stdout","text":"TensorFlow: 2.18.0\nGPUs: 1\n============================================================\nGPU OPTIMIZATION\n============================================================\nâœ“ 1 GPU(s) configured\nâœ“ XLA compilation enabled\nâœ“ Thread optimization enabled\n============================================================\nMixed precision policy: <DTypePolicy \"mixed_float16\">\n\nâœ“ Dataset: /kaggle/input/waffle/waffles_shuffled.jsonl\nâœ“ Embeddings: /kaggle/input/waffle/letter_embeddings.npy\n\n\n======================================================================\nTRAINING CONFIGURATION\n======================================================================\nPuzzles:             50\nActions:             441\nInput shape:         (21, 10)\nBatch size:          128\nHidden size:         256\nEpochs:              100\n======================================================================\n\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\nI0000 00:00:1763487299.658123      48 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15513 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"sequential\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\nâ”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\nâ”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\nâ”‚ conv1d (\u001b[38;5;33mConv1D\u001b[0m)                 â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m21\u001b[0m, \u001b[38;5;34m64\u001b[0m)         â”‚         \u001b[38;5;34m1,984\u001b[0m â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ batch_normalization             â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m21\u001b[0m, \u001b[38;5;34m64\u001b[0m)         â”‚           \u001b[38;5;34m256\u001b[0m â”‚\nâ”‚ (\u001b[38;5;33mBatchNormalization\u001b[0m)            â”‚                        â”‚               â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ conv1d_1 (\u001b[38;5;33mConv1D\u001b[0m)               â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m21\u001b[0m, \u001b[38;5;34m64\u001b[0m)         â”‚        \u001b[38;5;34m12,352\u001b[0m â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ batch_normalization_1           â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m21\u001b[0m, \u001b[38;5;34m64\u001b[0m)         â”‚           \u001b[38;5;34m256\u001b[0m â”‚\nâ”‚ (\u001b[38;5;33mBatchNormalization\u001b[0m)            â”‚                        â”‚               â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ conv1d_2 (\u001b[38;5;33mConv1D\u001b[0m)               â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m21\u001b[0m, \u001b[38;5;34m128\u001b[0m)        â”‚        \u001b[38;5;34m41,088\u001b[0m â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ batch_normalization_2           â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m21\u001b[0m, \u001b[38;5;34m128\u001b[0m)        â”‚           \u001b[38;5;34m512\u001b[0m â”‚\nâ”‚ (\u001b[38;5;33mBatchNormalization\u001b[0m)            â”‚                        â”‚               â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ flatten (\u001b[38;5;33mFlatten\u001b[0m)               â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2688\u001b[0m)           â”‚             \u001b[38;5;34m0\u001b[0m â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dense (\u001b[38;5;33mDense\u001b[0m)                   â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            â”‚       \u001b[38;5;34m688,384\u001b[0m â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dropout (\u001b[38;5;33mDropout\u001b[0m)               â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            â”‚             \u001b[38;5;34m0\u001b[0m â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            â”‚        \u001b[38;5;34m32,896\u001b[0m â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            â”‚             \u001b[38;5;34m0\u001b[0m â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m441\u001b[0m)            â”‚        \u001b[38;5;34m56,889\u001b[0m â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\nâ”ƒ<span style=\"font-weight: bold\"> Layer (type)                    </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape           </span>â”ƒ<span style=\"font-weight: bold\">       Param # </span>â”ƒ\nâ”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\nâ”‚ conv1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)                 â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">21</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         â”‚         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,984</span> â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ batch_normalization             â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">21</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            â”‚                        â”‚               â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ conv1d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">21</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">12,352</span> â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ batch_normalization_1           â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">21</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            â”‚                        â”‚               â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ conv1d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">21</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">41,088</span> â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ batch_normalization_2           â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">21</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            â”‚                        â”‚               â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2688</span>)           â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            â”‚       <span style=\"color: #00af00; text-decoration-color: #00af00\">688,384</span> â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">441</span>)            â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">56,889</span> â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m834,617\u001b[0m (3.18 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">834,617</span> (3.18 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m834,105\u001b[0m (3.18 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">834,105</span> (3.18 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m512\u001b[0m (2.00 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> (2.00 KB)\n</pre>\n"},"metadata":{}},{"name":"stderr","text":"I0000 00:00:1763487301.033807      48 cuda_dnn.cc:529] Loaded cuDNN version 90300\n","output_type":"stream"},{"name":"stdout","text":"\nâœ“ Model compiled with XLA\nâœ“ GPU warmup complete\n\n\nğŸš€ Starting training...\n\n","output_type":"stream"},{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1763487303.192169     110 service.cc:148] XLA service 0x7b251c006c80 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\nI0000 00:00:1763487303.192799     110 service.cc:156]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\nI0000 00:00:1763487303.292627     110 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","output_type":"stream"},{"name":"stdout","text":"Ep    1/100 | Loss:   4.4716 | Îµ: 0.9500 | Wins:    0 (  0.0%) | AvgR: -553.46\nEp    2/100 | Loss:   0.2331 | Îµ: 0.9025 | Wins:    0 (  0.0%) | AvgR: -540.73\nEp    3/100 | Loss:   0.2318 | Îµ: 0.8574 | Wins:    0 (  0.0%) | AvgR: -524.11\nEp    4/100 | Loss:   0.2311 | Îµ: 0.8145 | Wins:    0 (  0.0%) | AvgR: -526.49\nEp    5/100 | Loss:   0.2311 | Îµ: 0.7738 | Wins:    0 (  0.0%) | AvgR: -530.68\nEp    6/100 | Loss:   0.2312 | Îµ: 0.7351 | Wins:    0 (  0.0%) | AvgR: -523.40\nEp    7/100 | Loss:   0.2322 | Îµ: 0.6983 | Wins:    0 (  0.0%) | AvgR: -512.32\nEp    8/100 | Loss:   0.2315 | Îµ: 0.6634 | Wins:    0 (  0.0%) | AvgR: -506.18\nEp    9/100 | Loss:   0.2297 | Îµ: 0.6302 | Wins:    0 (  0.0%) | AvgR: -500.98\nEp   10/100 | Loss:   0.2289 | Îµ: 0.5987 | Wins:    0 (  0.0%) | AvgR: -493.64\nEp   20/100 | Loss:   0.2266 | Îµ: 0.3585 | Wins:    0 (  0.0%) | AvgR: -457.64\nEp   30/100 | Loss:   0.2229 | Îµ: 0.2146 | Wins:    0 (  0.0%) | AvgR: -435.58\nEp   40/100 | Loss:   0.2244 | Îµ: 0.1285 | Wins:    0 (  0.0%) | AvgR: -405.11\nEp   50/100 | Loss:   0.2264 | Îµ: 0.0769 | Wins:    0 (  0.0%) | AvgR: -394.07\n\n======================================================================\nCHECKPOINT - Epoch 50\nWin rate: 0.0% | Avg reward: -394.07\n======================================================================\n\nEp   60/100 | Loss:   0.2256 | Îµ: 0.0500 | Wins:    0 (  0.0%) | AvgR: -453.10\nEp   70/100 | Loss:   0.2239 | Îµ: 0.0500 | Wins:    0 (  0.0%) | AvgR: -440.24\nEp   80/100 | Loss:   0.2175 | Îµ: 0.0500 | Wins:    0 (  0.0%) | AvgR: -413.16\nEp   90/100 | Loss:   0.2303 | Îµ: 0.0500 | Wins:    0 (  0.0%) | AvgR: -397.83\nEp  100/100 | Loss:   0.2267 | Îµ: 0.0500 | Wins:    0 (  0.0%) | AvgR: -407.38\n\n======================================================================\nCHECKPOINT - Epoch 100\nWin rate: 0.0% | Avg reward: -407.38\n======================================================================\n\n\nâœ… Training complete! Win rate: 0.0%\n\nâœ… Model saved to /kaggle/working/model/\n","output_type":"stream"}],"execution_count":4}]}
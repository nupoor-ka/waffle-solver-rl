{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13830953,"sourceType":"datasetVersion","datasetId":8744658}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%writefile env.py\n\"\"\"\nWaffle Game Environment Definition Class - With Letter Embeddings (5×5×10 input)\n\"\"\"\n\nimport numpy as np\nimport json\nfrom typing import Tuple, List, Optional\n\nclass Waffle(object):\n    def __init__(self, puzzles_file: str = \"waffles_shuffled_2k.jsonl\", \n                 embeddings_file: str = \"letter_embeddings.npy\",\n                 vocab_file: str = \"/kaggle/input/waffle/words.txt\") -> None:\n        self.puzzles_file = puzzles_file\n        self.puzzles = self._load_puzzles()\n        self.embeddings = self._load_embeddings(embeddings_file)\n        self.grid_size = 21\n        self.current_puzzle = None\n        self.current_state = None\n        self.target_state = None\n        self.fixed_indices = None\n        \n        self.row_words = [\n            [0, 1, 2, 3, 4],\n            [8, 9, 10, 11, 12],\n            [16, 17, 18, 19, 20]\n        ]\n        self.col_words = [\n            [0, 5, 8, 13, 16],\n            [2, 6, 10, 14, 18],\n            [4, 7, 12, 15, 20]\n        ]\n\n        # Load vocabulary and precompute n-grams\n        self.vocab = self._load_vocab(vocab_file)\n        self.bigram_set = set()\n        self.trigram_set = set()\n        for word in self.vocab:\n            self.bigram_set.update([word[i:i+2] for i in range(len(word)-1)])\n            self.trigram_set.update([word[i:i+3] for i in range(len(word)-2)])\n\n        self.reset(0)\n\n    def _load_puzzles(self) -> List[dict]:\n        puzzles = []\n        with open(self.puzzles_file, \"r\") as f:\n            for line in f:\n                puzzles.append(json.loads(line))\n        return puzzles\n\n    def _load_vocab(self, vocab_file: str) -> set:\n        vocab = set()\n        with open(vocab_file, 'r') as f:\n            for line in f:\n                word = line.strip().lower()\n                if len(word) == 5:\n                    vocab.add(word)\n        print(f\"Loaded vocabulary with {len(vocab)} words\")\n        return vocab\n\n    def _load_embeddings(self, embeddings_file: str) -> np.ndarray:\n        try:\n            embeddings = np.load(embeddings_file)\n            print(f\"Loaded embeddings: {embeddings.shape}\")\n            return embeddings.astype(np.float32)\n        except FileNotFoundError:\n            print(f\"Creating random embeddings\")\n            return np.random.randn(27, 8).astype(np.float32)\n\n    def _string_to_array(self, flat21: str) -> np.ndarray:\n        return np.array([ord(c) - ord('a') + 1 for c in flat21], dtype=np.int32)\n\n    def _array_to_string(self, arr: np.ndarray) -> str:\n        return \"\".join([chr(int(x) + ord('a') - 1) for x in arr])\n\n    def _get_word_for_position(self, pos: int) -> Optional[List[int]]:\n        for word in self.row_words:\n            if pos in word:\n                return word\n        for word in self.col_words:\n            if pos in word:\n                return word\n        return None\n\n    def _calculate_colors_and_embeddings(self, state: np.ndarray) -> np.ndarray:\n        features = np.zeros((21, 10), dtype=np.float32)\n        \n        for i in range(21):\n            current_letter = state[i]\n            target_letter = self.target_state[i]\n            \n            if current_letter == target_letter:\n                features[i, 0] = 1.0  # Green channel\n            else:\n                word_positions = self._get_word_for_position(i)\n                if word_positions:\n                    target_word_letters = [self.target_state[p] for p in word_positions]\n                    if current_letter in target_word_letters:\n                        features[i, 1] = 1.0  # Yellow channel\n            \n            letter_idx = int(current_letter) - 1 if current_letter >= 1 else 26\n            features[i, 2:10] = self.embeddings[letter_idx]\n        \n        return features\n\n    def _count_correct_positions(self, state: np.ndarray) -> int:\n        return np.sum(state == self.target_state)\n        \n    def reset(self, index: int) -> None:\n        self.current_puzzle = self.puzzles[index]\n        self.target_state = self._string_to_array(self.current_puzzle[\"target_flat21\"])\n        self.current_state = self._string_to_array(self.current_puzzle[\"shuffled_flat21\"])\n        self.fixed_indices = set(self.current_puzzle[\"fixed_indices\"])\n        self.moves = 0\n        self.max_moves = 50\n        self.prev_correct_count = self._count_correct_positions(self.current_state)\n        self.prev_features = self._calculate_colors_and_embeddings(self.current_state)\n\n    def _update_state(self, action: int) -> None:\n        pos1 = action // 21\n        pos2 = action % 21\n        temp = self.current_state[pos1]\n        self.current_state[pos1] = self.current_state[pos2]\n        self.current_state[pos2] = temp\n        self.moves += 1\n\n    def _get_reward(self, action: int) -> float:\n        pos1 = action // 21\n        pos2 = action % 21\n        reward = -0.1\n        \n        if np.array_equal(self.current_state, self.target_state):\n            return 1000.0 + reward\n        \n        current_correct = self._count_correct_positions(self.current_state)\n        correct_change = current_correct - self.prev_correct_count\n        current_features = self._calculate_colors_and_embeddings(self.current_state)\n        \n        if self.current_state[pos1] == self.current_state[pos2]:\n            reward += -5\n        \n        if correct_change > 0:\n            reward += 3 * correct_change\n        elif correct_change < 0:\n            reward += -3 * abs(correct_change)\n        \n        for pos in [pos1, pos2]:\n            prev_green = self.prev_features[pos, 0]\n            prev_yellow = self.prev_features[pos, 1]\n            curr_green = current_features[pos, 0]\n            curr_yellow = current_features[pos, 1]\n            \n            if prev_green == 0 and prev_yellow == 0 and curr_yellow == 1.0:\n                reward += 0.5\n            elif prev_yellow == 1.0 and curr_green == 0 and curr_yellow == 0:\n                reward += -0.5\n            # elif prev_yellow == 1.0 and curr_green == 1.0:\n            #     reward += 3\n\n        current_letters = [chr(c + ord('a') - 1) for c in self.current_state]\n        \n        def is_word(s):\n            return s in self.vocab\n        \n        def count_valid_ngrams(s, ngram_set, n):\n            return sum(1 for i in range(len(s) - n + 1) if s[i:i+n] in ngram_set)\n        \n        for indices in self.row_words + self.col_words:\n            word_str = ''.join([current_letters[i] for i in indices])\n            if is_word(word_str):\n                reward += 5.0\n            reward += 0.5 * count_valid_ngrams(word_str, self.bigram_set, 2)\n            reward += 0.2 * count_valid_ngrams(word_str, self.trigram_set, 3)\n        \n        self.prev_correct_count = current_correct\n        self.prev_features = current_features\n        return reward\n\n    def _is_over(self) -> bool:\n        solved = np.array_equal(self.current_state, self.target_state)\n        max_moves_reached = self.moves >= self.max_moves\n        return solved or max_moves_reached\n    \n    def _is_win(self) -> bool:\n        return np.array_equal(self.current_state, self.target_state)\n\n    def observe(self) -> np.ndarray:\n        features = self._calculate_colors_and_embeddings(self.current_state)\n        grid = np.zeros((5, 5, 10), dtype=np.float32)\n\n        blank_embedding = self.embeddings[26]\n        blank_feature_vector = np.zeros(10, dtype=np.float32)\n        blank_feature_vector[2:10] = blank_embedding\n        blank_positions = [(1,1), (1,3), (3,1), (3,3)]\n        for r, c in blank_positions:\n            grid[r, c, :] = blank_feature_vector\n        \n        position_to_grid = [\n            (0,0), (0,1), (0,2), (0,3), (0,4),\n            (1,0), (1,2), (1,4),\n            (2,0), (2,1), (2,2), (2,3), (2,4),\n            (3,0), (3,2), (3,4),\n            (4,0), (4,1), (4,2), (4,3), (4,4)\n        ]\n        \n        for flat_idx in range(21):\n            r, c = position_to_grid[flat_idx]\n            grid[r, c, :] = features[flat_idx]\n        \n        return grid.reshape(1, 5, 5, 10)\n\n    def act(self, action: int) -> Tuple[np.ndarray, float, bool, bool]:\n        self._update_state(action)\n        reward = self._get_reward(action)\n        game_over = self._is_over()\n        game_win = self._is_win()\n        return self.observe(), reward, game_over, game_win\n\n    def get_valid_actions(self) -> List[int]:\n        valid_actions = []\n        for pos1 in range(21):\n            for pos2 in range(21):\n                if pos1 != pos2:\n                    action = pos1 * 21 + pos2\n                    valid_actions.append(action)\n        return valid_actions\n\n    def get_num_actions(self) -> int:\n        return 21 * 21\n\n    def get_current_state_letters(self) -> np.ndarray:\n        return self.current_state","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-22T18:20:56.604204Z","iopub.execute_input":"2025-11-22T18:20:56.604504Z","iopub.status.idle":"2025-11-22T18:20:56.630178Z","shell.execute_reply.started":"2025-11-22T18:20:56.604483Z","shell.execute_reply":"2025-11-22T18:20:56.629182Z"}},"outputs":[{"name":"stdout","text":"Writing env.py\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"%%writefile experience_replay.py\n\"\"\"\nWaffle Game Experience Replay Class - Double DQN without Action Masking\n\"\"\"\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom typing import Tuple\n\nclass ExperienceReplay(object):\n    def __init__(self, max_memory: int = 10000, discount: float = 0.99) -> None:\n        self.max_memory = max_memory\n        self.memory = list()\n        self.discount = discount\n\n    def add_experience(self, sars: list, game_over: bool, unmovable_positions = None) -> None:\n        \"\"\"\n        Store experience.\n        \n        Args:\n            sars: [state, action, reward, next_state]\n            game_over: Whether episode ended\n            unmovable_positions: Not used now\n        \"\"\"\n        self.memory.append([sars, game_over, unmovable_positions])\n        if len(self.memory) > self.max_memory:\n            del self.memory[0]\n\n    def get_qlearning_batch(\n        self, model: keras.Model, target_model: keras.Model, batch_size: int = 128\n    ) -> Tuple[np.ndarray, np.ndarray]:\n        \"\"\"\n        Generate training batch for Double DQN without action masking.\n        \n        Args:\n            model: Main Q-network (for action selection)\n            target_model: Target Q-network (for action evaluation)\n            batch_size: Number of experiences to sample\n            \n        Returns:\n            (states, target_q_values) for training\n        \"\"\"\n        memory_length = len(self.memory)\n        num_inputs = min(memory_length, batch_size)\n        ids = np.random.choice(memory_length, size=num_inputs, replace=False)\n        \n        # Unpack experiences\n        sars = list(zip(*[self.memory[id_][0] for id_ in ids]))\n        previous_states, action_ts, rewards, current_states = (\n            np.concatenate(e) if isinstance(e[0], np.ndarray) else np.stack(e)\n            for e in sars\n        )\n        game_over = np.array([self.memory[id_][1] for id_ in ids], dtype=bool)\n        \n        # Convert to tensors\n        previous_states_tensor = tf.constant(previous_states, dtype=tf.float32)\n        current_states_tensor = tf.constant(current_states, dtype=tf.float32)\n        \n        # Get current Q-values for previous states\n        targets = model(previous_states_tensor, training=False).numpy()\n        \n        # DOUBLE DQN:\n        # Step 1: Main network selects best action for next state (no masking)\n        next_q_main = model(current_states_tensor, training=False).numpy()\n        best_actions = np.argmax(next_q_main, axis=1)\n        \n        # Step 2: Target network evaluates the chosen actions\n        next_q_target = target_model(current_states_tensor, training=False).numpy()\n        Q_sa = next_q_target[np.arange(num_inputs), best_actions]\n        \n        # Bellman update\n        targets[np.arange(num_inputs), action_ts] = (\n            rewards + self.discount * Q_sa * (~game_over)\n        )\n        \n        return previous_states, targets\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-22T18:20:56.760638Z","iopub.execute_input":"2025-11-22T18:20:56.760897Z","iopub.status.idle":"2025-11-22T18:20:56.766966Z","shell.execute_reply.started":"2025-11-22T18:20:56.760876Z","shell.execute_reply":"2025-11-22T18:20:56.766300Z"}},"outputs":[{"name":"stdout","text":"Writing experience_replay.py\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# Usually not needed, TensorFlow is pre-installed on Kaggle\nimport tensorflow as tf\nprint(f\"TensorFlow version: {tf.__version__}\")\nprint(f\"GPUs available: {len(tf.config.list_physical_devices('GPU'))}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-22T18:20:56.768057Z","iopub.execute_input":"2025-11-22T18:20:56.768326Z","iopub.status.idle":"2025-11-22T18:21:15.081381Z","shell.execute_reply.started":"2025-11-22T18:20:56.768309Z","shell.execute_reply":"2025-11-22T18:21:15.080532Z"}},"outputs":[{"name":"stderr","text":"2025-11-22 18:20:58.484609: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1763835658.714007      48 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1763835658.784462      48 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"name":"stdout","text":"TensorFlow version: 2.18.0\nGPUs available: 2\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import os\nimport json\nimport numpy as np\nimport logging\n\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Conv2D, Flatten, Dropout\nfrom tensorflow.keras.optimizers import Adam\n\nfrom experience_replay import ExperienceReplay\nfrom env import Waffle\n\n# GPU Configuration\nprint(\"=\"*60)\nprint(\"SYSTEM CHECK\")\nprint(\"=\"*60)\ngpus = tf.config.list_physical_devices('GPU')\nif gpus:\n    try:\n        for gpu in gpus:\n            tf.config.experimental.set_memory_growth(gpu, True)\n        print(f\"✓ {len(gpus)} GPU(s) configured\")\n    except RuntimeError as e:\n        print(f\"GPU warning (ignorable): {e}\")\nelse:\n    print(\"No GPU\")\nprint(\"=\"*60)\n\n# Find dataset\ndataset_path = None\nembeddings_path = '/kaggle/input/letter_embeddings.npy'\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        if filename == 'waffles_shuffled_2k.jsonl':\n            dataset_path = os.path.join(dirname, filename)\n            print(f\"\\n✓ Found dataset: {dataset_path}\")\n        if filename == 'letter_embeddings.npy':\n            embeddings_path = os.path.join(dirname, filename)\n            print(f\"✓ Found embeddings: {embeddings_path}\")\n\nif not dataset_path:\n    raise FileNotFoundError(\"waffles_shuffled_2k.jsonl not found\")\n\n# Model Definition - Conv2D for 5×5 grid\ndef define_model(input_shape, hidden_size, num_actions, \n                learning_rate=0.001, conv_filters=64, loss=\"mse\"):\n    policy = tf.keras.mixed_precision.Policy('mixed_float16')\n    tf.keras.mixed_precision.set_global_policy(policy)\n    \n    model = Sequential()\n    model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', \n                     padding='same', input_shape=input_shape, dtype='float32'))\n    model.add(Conv2D(64, kernel_size=(3, 3), activation='relu', padding='same'))\n    model.add(Flatten())\n    model.add(Dense(256, activation='relu'))\n    model.add(Dropout(0.2))\n    model.add(Dense(128, activation='relu'))\n    model.add(Dense(num_actions, dtype='float32'))\n    model.compile(Adam(learning_rate=learning_rate), loss)\n    return model\n\ndef train_model(model, target_model, env, epochs, experience_replay, \n               epsilon_start, epsilon_end, epsilon_decay, batch_size, \n               puzzles_per_epoch, target_update_freq):\n    import numpy as np\n    import logging\n    logging.info(\"Starting training...\")\n    win_count = 0\n    total_rewards = []\n    epsilon = epsilon_start\n    num_actions = env.get_num_actions()\n    training_steps = 0\n    \n    # Precompute initial correctness for all puzzles\n    initial_correctness_list = []\n    for idx, p in enumerate(env.puzzles):\n        state = env._string_to_array(p[\"shuffled_flat21\"])\n        target = env._string_to_array(p[\"target_flat21\"])\n        correct = np.sum(state == target)\n        initial_correctness_list.append((idx, correct))\n\n    bins = {\n        'easy':   [idx for idx, c in initial_correctness_list if c >= 9],\n        'medium': [idx for idx, c in initial_correctness_list if 7 <= c < 9],\n        'hard':   [idx for idx, c in initial_correctness_list if 5 <= c < 7],\n        'harder': [idx for idx, c in initial_correctness_list if c < 5]\n    }\n\n    def sample_puzzles(epoch):\n        # Divide training epochs into four phases\n        fraction = epoch / epochs\n        if fraction < 0.25:\n            return np.random.choice(bins['easy'], size=puzzles_per_epoch, replace=True)\n        elif fraction < 0.50:\n            mixed = bins['easy'] + bins['medium']\n            return np.random.choice(mixed, size=puzzles_per_epoch, replace=True)\n        elif fraction < 0.75:\n            mixed = bins['medium'] + bins['hard']\n            return np.random.choice(mixed, size=puzzles_per_epoch, replace=True)\n        else:\n            mixed = bins['hard'] + bins['harder']\n            return np.random.choice(mixed, size=puzzles_per_epoch, replace=True)\n    \n    @tf.function\n    def predict_q(state):\n        return model(state, training=False)\n    \n    for epoch in range(epochs):\n        loss = 0.0\n        final_correct_positions = []\n        num_moves = []\n        epoch_wins = 0\n        \n        puzzle_indices = sample_puzzles(epoch)\n        \n        for puzzle_idx in puzzle_indices:\n            env.reset(puzzle_idx)\n            current_state = env.observe()\n            game_over = False\n            episode_reward = 0\n            step_count = 0\n\n            if np.array_equal(env.current_state, env.target_state):\n                print(f\"Puzzle {puzzle_idx} is already solved at reset!\")\n\n            initial_correct = env._count_correct_positions(env.current_state)\n            if initial_correct == 21:\n                print(f\"Puzzle {puzzle_idx} has all 21 correct at start!\")\n            \n            fixed_positions = env.fixed_indices\n\n            patience = 20\n            no_progress_steps = 0\n            prev_green_count = env._count_correct_positions(env.current_state)\n            \n            while not game_over:\n                current_green_count = env._count_correct_positions(env.current_state)\n                if current_green_count > prev_green_count:\n                    no_progress_steps = 0\n                    prev_green_count = current_green_count\n                else:\n                    no_progress_steps += 1\n\n                if no_progress_steps >= patience:\n                    # Early stop episode due to stagnation\n                    game_over = True\n                    # Optional: penalty reward for failing to progress\n                    reward -= 10.0  \n                    # Store experience for final state\n                \n                previous_state = current_state\n                movable_positions = [pos for pos in range(21) if pos not in fixed_positions]\n                valid_actions = []\n                for i, pos1 in enumerate(movable_positions):\n                    for pos2 in movable_positions[i+1:]:\n                        action = pos1 * 21 + pos2\n                        valid_actions.append(action)\n                \n                if np.random.rand() <= epsilon:\n                    action = np.random.choice(valid_actions)\n                else:\n                    q = predict_q(tf.constant(previous_state, dtype=tf.float32)).numpy()[0]\n                    masked_q = np.full(num_actions, -np.inf)\n                    masked_q[valid_actions] = q[valid_actions]\n                    action = int(np.argmax(masked_q))\n\n                current_state, reward, game_over, game_win = env.act(action)\n                episode_reward += reward\n                step_count += 1\n                \n                experience_replay.add_experience(\n                    [previous_state, int(action), reward, current_state], \n                    game_over,\n                    fixed_positions\n                )\n                \n                if len(experience_replay.memory) >= batch_size and step_count % 5 == 0:\n                    inputs, targets = experience_replay.get_qlearning_batch(\n                        model, target_model, batch_size=batch_size\n                    )\n                    loss += model.train_on_batch(inputs, targets)\n                    training_steps += 1\n                    \n                    if training_steps % target_update_freq == 0:\n                        target_model.set_weights(model.get_weights())\n                        print(f\"  [Target network updated at step {training_steps}]\")\n            \n            total_rewards.append(episode_reward)\n            final_correct = env._count_correct_positions(env.current_state)\n            final_correct_positions.append(final_correct)\n            num_moves.append(step_count)\n            if game_win:\n                epoch_wins += 1\n        \n        win_count += epoch_wins\n        epsilon = max(epsilon_end, epsilon * epsilon_decay)\n        \n        if (epoch + 1) % 10 == 0 or epoch < 10:\n            recent_window = min(400, len(total_rewards))\n            avg_reward = np.mean(total_rewards[-recent_window:])\n            win_rate = win_count / ((epoch + 1) * puzzles_per_epoch) * 100\n            avg_correct = np.mean(final_correct_positions[-recent_window:])\n            avg_moves = np.mean(num_moves[-recent_window:])\n            print(f\"Ep {epoch+1:4d}/{epochs} | Loss: {loss:8.4f} | ε: {epsilon:.4f} | \"\n                  f\"Wins: {win_count} ({win_rate:5.2f}%) | AvgR: {avg_reward:7.2f} | AvgCorrect: {avg_correct:.1f}/21 | AvgMoves: {avg_moves:.1f}\")\n        \n        if (epoch + 1) % 50 == 0:\n            print(f\"\\n{'='*70}\")\n            print(f\"CHECKPOINT - Epoch {epoch+1}/{epochs}\")\n            print(f\"Total wins: {win_count}/{(epoch+1)*puzzles_per_epoch} ({win_rate:.2f}%)\")\n            print(f\"Avg reward (recent): {avg_reward:.2f}\")\n            print(f\"Avg correct (recent): {avg_correct:.1f}/21\")\n            print(f\"Training steps: {training_steps}\")\n            print(f\"{'='*70}\\n\")\n\n            checkpoint_dir = '/kaggle/working/model_checkpoints'\n            os.makedirs(checkpoint_dir, exist_ok=True)\n            \n            model.save_weights(f'{checkpoint_dir}/model_epoch_{epoch+1}.weights.h5')\n            target_model.save_weights(f'{checkpoint_dir}/target_model_epoch_{epoch+1}.weights.h5')\n            with open(f'{checkpoint_dir}/model_epoch_{epoch+1}.json', 'w') as f:\n                json.dump(model.to_json(), f)\n            print(f\"✓ Saved checkpoint at epoch {epoch+1}\")\n    \n    total_episodes = epochs * puzzles_per_epoch\n    final_win_rate = win_count / total_episodes * 100\n    print(f\"\\nTraining complete!\")\n    print(f\"Total wins: {win_count}/{total_episodes} ({final_win_rate:.2f}%)\")\n    return model\n\n# Hyperparameters\nEPOCHS = 40\nEPSILON_START = 1.0\nEPSILON_END = 0.01\nTARGET_EPSILON_EPOCH = 100  # Reach epsilon_end by this epoch\nEPSILON_DECAY = (EPSILON_END / EPSILON_START) ** (1 / TARGET_EPSILON_EPOCH)\nMAX_MEMORY = 10000\nHIDDEN_SIZE = 256  # Reduced from 512\nCONV_FILTERS = 64\nBATCH_SIZE = 128\nDISCOUNT = 0.99\nLEARNING_RATE = 0.001  # Increased from 0.0005\nPUZZLES_PER_EPOCH = 400\nTARGET_UPDATE_FREQUENCY = 1000\n\nprint(f\"\\n{'='*70}\")\nprint(f\"EPSILON SCHEDULE\")\nprint(f\"{'='*70}\")\nprint(f\"Start: {EPSILON_START}\")\nprint(f\"End: {EPSILON_END}\")\nprint(f\"Target epoch: {TARGET_EPSILON_EPOCH}\")\nprint(f\"Decay rate: {EPSILON_DECAY:.6f}\")\nprint(f\"{'='*70}\")\n\n# Initialize Environment\nenv = Waffle(puzzles_file=dataset_path, embeddings_file=embeddings_path)\nnum_actions = env.get_num_actions()\ninput_shape = (5, 5, 10)\n\nprint(f\"\\n{'='*70}\")\nprint(f\"ENVIRONMENT SETUP\")\nprint(f\"{'='*70}\")\nprint(f\"Puzzles loaded:      {len(env.puzzles)}\")\nprint(f\"Puzzles per epoch:   {PUZZLES_PER_EPOCH}\")\nprint(f\"Total episodes:      {EPOCHS * PUZZLES_PER_EPOCH}\")\nprint(f\"Action space:        {num_actions}\")\nprint(f\"Input shape:         {input_shape} (5×5 grid × 10 features)\")\nprint(f\"{'='*70}\\n\")\n\nprint(\"=\"*70)\nprint(\"PUZZLE VALIDATION CHECK (First 10 puzzles)\")\nprint(\"=\"*70)\nfor i in range(min(10, len(env.puzzles))):\n    env.reset(i)\n    initial_correct = env._count_correct_positions(env.current_state)\n    is_solved = np.array_equal(env.current_state, env.target_state)\n    num_fixed = len(env.fixed_indices)\n    valid_actions = len(env.get_valid_actions())\n    \n    status = \"✓ OK\" if not is_solved else \"PRE-SOLVED\"\n    print(f\"Puzzle {i:3d}: Correct={initial_correct:2d}/21, Fixed={num_fixed:2d}, \"\n          f\"ValidActions={valid_actions:3d} | {status}\")\n\n\n# Build Main Model\nprint(\"Building main model...\")\nmodel = define_model(\n    input_shape=input_shape,\n    hidden_size=HIDDEN_SIZE,\n    num_actions=num_actions,\n    learning_rate=LEARNING_RATE,\n    conv_filters=CONV_FILTERS\n)\nmodel.summary()\n\n# Build Target Model (Double DQN)\nprint(\"\\nBuilding target model...\")\ntarget_model = define_model(\n    input_shape=input_shape,\n    hidden_size=HIDDEN_SIZE,\n    num_actions=num_actions,\n    learning_rate=LEARNING_RATE,\n    conv_filters=CONV_FILTERS\n)\ntarget_model.set_weights(model.get_weights())\nprint(\"✓ Target network initialized with main network weights\")\n\n# GPU Test\ntest_input = tf.random.normal([1, 5, 5, 10])\ntest_output = model(test_input, training=False)\nprint(f\"\\n✓ GPU test passed\")\nprint(f\"  Input: {test_input.device}\")\nprint(f\"  Output: {test_output.device}\\n\")\n\n# Train\nexp_replay = ExperienceReplay(max_memory=MAX_MEMORY, discount=DISCOUNT)\nlogging.basicConfig(level=logging.INFO)\n\ntrained_model = train_model(\n    model=model,\n    target_model=target_model,\n    env=env,\n    epochs=EPOCHS,\n    experience_replay=exp_replay,\n    epsilon_start=EPSILON_START,\n    epsilon_end=EPSILON_END,\n    epsilon_decay=EPSILON_DECAY,\n    batch_size=BATCH_SIZE,\n    puzzles_per_epoch=PUZZLES_PER_EPOCH,\n    target_update_freq=TARGET_UPDATE_FREQUENCY\n)\n\n# Save Models\noutput_dir = '/kaggle/working/model'\nos.makedirs(output_dir, exist_ok=True)\n\ntrained_model.save_weights(f'{output_dir}/model.weights.h5', overwrite=True)\ntarget_model.save_weights(f'{output_dir}/target_model.weights.h5', overwrite=True)\nwith open(f'{output_dir}/model.json', 'w') as f:\n    json.dump(trained_model.to_json(), f)\n\nprint(f\"\\n✓ Models saved to {output_dir}/\")\nprint(f\"  - model.weights.h5 (main network)\")\nprint(f\"  - target_model.weights.h5 (target network)\")\nprint(f\"  - model.json (architecture)\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-22T18:21:15.082432Z","iopub.execute_input":"2025-11-22T18:21:15.082948Z","iopub.status.idle":"2025-11-22T19:07:46.266563Z","shell.execute_reply.started":"2025-11-22T18:21:15.082929Z","shell.execute_reply":"2025-11-22T19:07:46.265377Z"}},"outputs":[{"name":"stdout","text":"============================================================\nSYSTEM CHECK\n============================================================\n✓ 2 GPU(s) configured\n============================================================\n\n✓ Found dataset: /kaggle/input/waffle/waffles_shuffled_2k.jsonl\n✓ Found embeddings: /kaggle/input/waffle/letter_embeddings.npy\n\n======================================================================\nEPSILON SCHEDULE\n======================================================================\nStart: 1.0\nEnd: 0.01\nTarget epoch: 100\nDecay rate: 0.954993\n======================================================================\nLoaded embeddings: (27, 8)\nLoaded vocabulary with 5757 words\n\n======================================================================\nENVIRONMENT SETUP\n======================================================================\nPuzzles loaded:      2000\nPuzzles per epoch:   400\nTotal episodes:      16000\nAction space:        441\nInput shape:         (5, 5, 10) (5×5 grid × 10 features)\n======================================================================\n\n======================================================================\nPUZZLE VALIDATION CHECK (First 10 puzzles)\n======================================================================\nPuzzle   0: Correct= 8/21, Fixed= 5, ValidActions=420 | ✓ OK\nPuzzle   1: Correct= 9/21, Fixed= 5, ValidActions=420 | ✓ OK\nPuzzle   2: Correct= 6/21, Fixed= 5, ValidActions=420 | ✓ OK\nPuzzle   3: Correct= 7/21, Fixed= 5, ValidActions=420 | ✓ OK\nPuzzle   4: Correct= 6/21, Fixed= 5, ValidActions=420 | ✓ OK\nPuzzle   5: Correct= 7/21, Fixed= 5, ValidActions=420 | ✓ OK\nPuzzle   6: Correct= 6/21, Fixed= 5, ValidActions=420 | ✓ OK\nPuzzle   7: Correct= 6/21, Fixed= 5, ValidActions=420 | ✓ OK\nPuzzle   8: Correct= 6/21, Fixed= 5, ValidActions=420 | ✓ OK\nPuzzle   9: Correct= 9/21, Fixed= 5, ValidActions=420 | ✓ OK\nBuilding main model...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\nI0000 00:00:1763835675.488974      48 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13942 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\nI0000 00:00:1763835675.489714      48 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 13942 MB memory:  -> device: 1, name: Tesla T4, pci bus id: 0000:00:05.0, compute capability: 7.5\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"sequential\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │         \u001b[38;5;34m2,912\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │        \u001b[38;5;34m18,496\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1600\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │       \u001b[38;5;34m409,856\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m32,896\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m441\u001b[0m)            │        \u001b[38;5;34m56,889\u001b[0m │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,912</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1600</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">409,856</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">441</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">56,889</span> │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m521,049\u001b[0m (1.99 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">521,049</span> (1.99 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m521,049\u001b[0m (1.99 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">521,049</span> (1.99 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}},{"name":"stdout","text":"\nBuilding target model...\n✓ Target network initialized with main network weights\n","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1763835677.001676      48 cuda_dnn.cc:529] Loaded cuDNN version 90300\nINFO:root:Starting training...\n","output_type":"stream"},{"name":"stdout","text":"\n✓ GPU test passed\n  Input: /job:localhost/replica:0/task:0/device:GPU:0\n  Output: /job:localhost/replica:0/task:0/device:GPU:0\n\n","output_type":"stream"},{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1763835680.016056      48 service.cc:148] XLA service 0x2b7c4de0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\nI0000 00:00:1763835680.017161      48 service.cc:156]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\nI0000 00:00:1763835680.017180      48 service.cc:156]   StreamExecutor device (1): Tesla T4, Compute Capability 7.5\nI0000 00:00:1763835682.950282      48 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","output_type":"stream"},{"name":"stdout","text":"  [Target network updated at step 1000]\n  [Target network updated at step 2000]\n  [Target network updated at step 3000]\nEp    1/40 | Loss: 249252.5627 | ε: 0.9550 | Wins: 0 ( 0.00%) | AvgR:  650.72 | AvgCorrect: 7.2/21 | AvgMoves: 50.0\n  [Target network updated at step 4000]\n  [Target network updated at step 5000]\n  [Target network updated at step 6000]\n  [Target network updated at step 7000]\nEp    2/40 | Loss: 32146.4060 | ε: 0.9120 | Wins: 0 ( 0.00%) | AvgR:  652.17 | AvgCorrect: 7.2/21 | AvgMoves: 50.0\n  [Target network updated at step 8000]\n  [Target network updated at step 9000]\n  [Target network updated at step 10000]\n  [Target network updated at step 11000]\nEp    3/40 | Loss: 19098.3089 | ε: 0.8710 | Wins: 0 ( 0.00%) | AvgR:  652.18 | AvgCorrect: 7.2/21 | AvgMoves: 50.0\n  [Target network updated at step 12000]\n  [Target network updated at step 13000]\n  [Target network updated at step 14000]\n  [Target network updated at step 15000]\nEp    4/40 | Loss: 14115.0567 | ε: 0.8318 | Wins: 0 ( 0.00%) | AvgR:  652.20 | AvgCorrect: 7.2/21 | AvgMoves: 50.0\n  [Target network updated at step 16000]\n  [Target network updated at step 17000]\n  [Target network updated at step 18000]\n  [Target network updated at step 19000]\nEp    5/40 | Loss: 11813.0984 | ε: 0.7943 | Wins: 0 ( 0.00%) | AvgR:  650.95 | AvgCorrect: 7.2/21 | AvgMoves: 50.0\n  [Target network updated at step 20000]\n  [Target network updated at step 21000]\n  [Target network updated at step 22000]\n  [Target network updated at step 23000]\nEp    6/40 | Loss: 10800.4002 | ε: 0.7586 | Wins: 0 ( 0.00%) | AvgR:  649.33 | AvgCorrect: 7.2/21 | AvgMoves: 50.0\n  [Target network updated at step 24000]\n  [Target network updated at step 25000]\n  [Target network updated at step 26000]\n  [Target network updated at step 27000]\nEp    7/40 | Loss: 11181.0845 | ε: 0.7244 | Wins: 0 ( 0.00%) | AvgR:  653.36 | AvgCorrect: 7.2/21 | AvgMoves: 50.0\n  [Target network updated at step 28000]\n  [Target network updated at step 29000]\n  [Target network updated at step 30000]\n  [Target network updated at step 31000]\nEp    8/40 | Loss: 11590.3673 | ε: 0.6918 | Wins: 0 ( 0.00%) | AvgR:  651.21 | AvgCorrect: 7.2/21 | AvgMoves: 50.0\n  [Target network updated at step 32000]\n  [Target network updated at step 33000]\n  [Target network updated at step 34000]\n  [Target network updated at step 35000]\nEp    9/40 | Loss: 12064.1380 | ε: 0.6607 | Wins: 0 ( 0.00%) | AvgR:  647.87 | AvgCorrect: 7.2/21 | AvgMoves: 50.0\n  [Target network updated at step 36000]\n  [Target network updated at step 37000]\n  [Target network updated at step 38000]\n  [Target network updated at step 39000]\nEp   10/40 | Loss: 12790.8869 | ε: 0.6310 | Wins: 0 ( 0.00%) | AvgR:  651.80 | AvgCorrect: 7.2/21 | AvgMoves: 50.0\n  [Target network updated at step 40000]\n  [Target network updated at step 41000]\n  [Target network updated at step 42000]\n  [Target network updated at step 43000]\n  [Target network updated at step 44000]\n  [Target network updated at step 45000]\n  [Target network updated at step 46000]\n  [Target network updated at step 47000]\n  [Target network updated at step 48000]\n  [Target network updated at step 49000]\n  [Target network updated at step 50000]\n  [Target network updated at step 51000]\n  [Target network updated at step 52000]\n  [Target network updated at step 53000]\n  [Target network updated at step 54000]\n  [Target network updated at step 55000]\n  [Target network updated at step 56000]\n  [Target network updated at step 57000]\n  [Target network updated at step 58000]\n  [Target network updated at step 59000]\n  [Target network updated at step 60000]\n  [Target network updated at step 61000]\n  [Target network updated at step 62000]\n  [Target network updated at step 63000]\n  [Target network updated at step 64000]\n  [Target network updated at step 65000]\n  [Target network updated at step 66000]\n  [Target network updated at step 67000]\n  [Target network updated at step 68000]\n  [Target network updated at step 69000]\n  [Target network updated at step 70000]\n  [Target network updated at step 71000]\n  [Target network updated at step 72000]\n  [Target network updated at step 73000]\n  [Target network updated at step 74000]\n  [Target network updated at step 75000]\n  [Target network updated at step 76000]\n  [Target network updated at step 77000]\n  [Target network updated at step 78000]\n  [Target network updated at step 79000]\nEp   20/40 | Loss: 25631.2533 | ε: 0.3981 | Wins: 0 ( 0.00%) | AvgR:  658.89 | AvgCorrect: 7.2/21 | AvgMoves: 50.0\n  [Target network updated at step 80000]\n  [Target network updated at step 81000]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_48/3289122333.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasicConfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mINFO\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m trained_model = train_model(\n\u001b[0m\u001b[1;32m    325\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m     \u001b[0mtarget_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_48/3289122333.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, target_model, env, epochs, experience_replay, epsilon_start, epsilon_end, epsilon_decay, batch_size, puzzles_per_epoch, target_update_freq)\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexperience_replay\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mstep_count\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m5\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m                     inputs, targets = experience_replay.get_qlearning_batch(\n\u001b[0m\u001b[1;32m    184\u001b[0m                         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m                     )\n","\u001b[0;32m/kaggle/working/experience_replay.py\u001b[0m in \u001b[0;36mget_qlearning_batch\u001b[0;34m(self, model, target_model, batch_size)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;31m# Get current Q-values for previous states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprevious_states_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0;31m# DOUBLE DQN:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    904\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mnew_scope\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mnew_scope\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 906\u001b[0;31m                         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    907\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    908\u001b[0m                     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/ops/operation.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     44\u001b[0m                 \u001b[0mobject_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{self.__class__.__name__}.call()\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m             )\n\u001b[0;32m---> 46\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;31m# Plain flow.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0mbound_signature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_keras_call_info_injected\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/models/sequential.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_functional\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_functional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0;31m# Fallback: Just apply the layer sequence.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/models/functional.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    180\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m                     \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_keras_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m         outputs = self._run_through_graph(\n\u001b[0m\u001b[1;32m    183\u001b[0m             \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperation_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0moperation_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/ops/function.py\u001b[0m in \u001b[0;36m_run_through_graph\u001b[0;34m(self, inputs, operation_fn, call_fn)\u001b[0m\n\u001b[1;32m    169\u001b[0m                     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m                     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m                 \u001b[0;31m# Update tensor_dict.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/models/functional.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    635\u001b[0m         ):\n\u001b[1;32m    636\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"training\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 637\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0moperation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    638\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    639\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    906\u001b[0m                         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 908\u001b[0;31m                     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    909\u001b[0m                 \u001b[0;31m# Change the layout for the layer output if needed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    910\u001b[0m                 \u001b[0;31m# This is useful for relayout intermediate tensor in the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/ops/operation.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     44\u001b[0m                 \u001b[0mobject_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{self.__class__.__name__}.call()\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m             )\n\u001b[0;32m---> 46\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;31m# Plain flow.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0mbound_signature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_keras_call_info_injected\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, training)\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/ops/numpy.py\u001b[0m in \u001b[0;36madd\u001b[0;34m(x1, x2)\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0many_symbolic_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mAdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msymbolic_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/sparse.py\u001b[0m in \u001b[0;36msparse_wrapper\u001b[0;34m(x1, x2)\u001b[0m\n\u001b[1;32m    491\u001b[0m                 \u001b[0;31m# x2 is an IndexedSlices, densify.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m                 \u001b[0mx2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msparse_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/numpy.py\u001b[0m in \u001b[0;36madd\u001b[0;34m(x1, x2)\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0mx2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias_add\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mop_dispatch_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1258\u001b[0m       \u001b[0;31m# Fallback dispatch system (dispatch v1):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1260\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdispatch_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1261\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1262\u001b[0m         \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/ops/nn_ops.py\u001b[0m in \u001b[0;36mbias_add\u001b[0;34m(value, bias, data_format, name)\u001b[0m\n\u001b[1;32m   3558\u001b[0m       \u001b[0mbias\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"bias\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3559\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3560\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgen_nn_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias_add\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3561\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3562\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/ops/gen_nn_ops.py\u001b[0m in \u001b[0;36mbias_add\u001b[0;34m(value, bias, data_format, name)\u001b[0m\n\u001b[1;32m    873\u001b[0m       \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    874\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 875\u001b[0;31m       return bias_add_eager_fallback(\n\u001b[0m\u001b[1;32m    876\u001b[0m           value, bias, data_format=data_format, name=name, ctx=_ctx)\n\u001b[1;32m    877\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_SymbolicException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/ops/gen_nn_ops.py\u001b[0m in \u001b[0;36mbias_add_eager_fallback\u001b[0;34m(value, bias, data_format, name, ctx)\u001b[0m\n\u001b[1;32m    896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 898\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mbias_add_eager_fallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAnnotated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTV_BiasAdd_T\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAnnotated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTV_BiasAdd_T\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_format\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAnnotated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTV_BiasAdd_T\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    899\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mdata_format\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    900\u001b[0m     \u001b[0mdata_format\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"NHWC\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":4},{"cell_type":"code","source":"def visualize_episode(model, env, puzzle_index=0, epsilon=0.0, max_steps=30):\n    env.reset(puzzle_index)\n    state = env.observe()\n    total_reward = 0\n    \n    for step in range(max_steps):\n        q_values = model(tf.constant(state, dtype=tf.float32)).numpy()[0]\n        valid_actions = env.get_valid_actions()\n        \n        masked_q = np.full(env.get_num_actions(), -np.inf)\n        masked_q[valid_actions] = q_values[valid_actions]\n        action = int(np.argmax(masked_q))\n        \n        state, reward, done, win = env.act(action)\n        total_reward += reward\n        \n        print(f\"Step {step+1}: Action={action}, Reward={reward:.2f}, Done={done}, Win={win}\")\n        print(f\"Current state letters: {env.get_current_state_letters()}\")\n        \n        if done:\n            print(f\"Episode finished after {step+1} steps with total reward {total_reward:.2f}\")\n            break\n\nvisualize_episode(model, env, puzzle_index=0, epsilon=0.0, max_steps=30)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-22T19:18:28.928335Z","iopub.execute_input":"2025-11-22T19:18:28.929058Z","iopub.status.idle":"2025-11-22T19:18:29.230707Z","shell.execute_reply.started":"2025-11-22T19:18:28.929016Z","shell.execute_reply":"2025-11-22T19:18:29.230088Z"}},"outputs":[{"name":"stdout","text":"Step 1: Action=244, Reward=7.40, Done=False, Win=False\nCurrent state letters: [14  1  1 14 15 11  1 20 19 20  6  9 19  5  1  9  5  1 14 20 19]\nStep 2: Action=244, Reward=19.40, Done=False, Win=False\nCurrent state letters: [14  1  1 14 15 11  1 20 19 20  6  5 19  9  1  9  5  1 14 20 19]\nStep 3: Action=244, Reward=7.40, Done=False, Win=False\nCurrent state letters: [14  1  1 14 15 11  1 20 19 20  6  9 19  5  1  9  5  1 14 20 19]\nStep 4: Action=244, Reward=19.40, Done=False, Win=False\nCurrent state letters: [14  1  1 14 15 11  1 20 19 20  6  5 19  9  1  9  5  1 14 20 19]\nStep 5: Action=244, Reward=7.40, Done=False, Win=False\nCurrent state letters: [14  1  1 14 15 11  1 20 19 20  6  9 19  5  1  9  5  1 14 20 19]\nStep 6: Action=244, Reward=19.40, Done=False, Win=False\nCurrent state letters: [14  1  1 14 15 11  1 20 19 20  6  5 19  9  1  9  5  1 14 20 19]\nStep 7: Action=244, Reward=7.40, Done=False, Win=False\nCurrent state letters: [14  1  1 14 15 11  1 20 19 20  6  9 19  5  1  9  5  1 14 20 19]\nStep 8: Action=244, Reward=19.40, Done=False, Win=False\nCurrent state letters: [14  1  1 14 15 11  1 20 19 20  6  5 19  9  1  9  5  1 14 20 19]\nStep 9: Action=244, Reward=7.40, Done=False, Win=False\nCurrent state letters: [14  1  1 14 15 11  1 20 19 20  6  9 19  5  1  9  5  1 14 20 19]\nStep 10: Action=244, Reward=19.40, Done=False, Win=False\nCurrent state letters: [14  1  1 14 15 11  1 20 19 20  6  5 19  9  1  9  5  1 14 20 19]\nStep 11: Action=244, Reward=7.40, Done=False, Win=False\nCurrent state letters: [14  1  1 14 15 11  1 20 19 20  6  9 19  5  1  9  5  1 14 20 19]\nStep 12: Action=244, Reward=19.40, Done=False, Win=False\nCurrent state letters: [14  1  1 14 15 11  1 20 19 20  6  5 19  9  1  9  5  1 14 20 19]\nStep 13: Action=244, Reward=7.40, Done=False, Win=False\nCurrent state letters: [14  1  1 14 15 11  1 20 19 20  6  9 19  5  1  9  5  1 14 20 19]\nStep 14: Action=244, Reward=19.40, Done=False, Win=False\nCurrent state letters: [14  1  1 14 15 11  1 20 19 20  6  5 19  9  1  9  5  1 14 20 19]\nStep 15: Action=244, Reward=7.40, Done=False, Win=False\nCurrent state letters: [14  1  1 14 15 11  1 20 19 20  6  9 19  5  1  9  5  1 14 20 19]\nStep 16: Action=244, Reward=19.40, Done=False, Win=False\nCurrent state letters: [14  1  1 14 15 11  1 20 19 20  6  5 19  9  1  9  5  1 14 20 19]\nStep 17: Action=244, Reward=7.40, Done=False, Win=False\nCurrent state letters: [14  1  1 14 15 11  1 20 19 20  6  9 19  5  1  9  5  1 14 20 19]\nStep 18: Action=244, Reward=19.40, Done=False, Win=False\nCurrent state letters: [14  1  1 14 15 11  1 20 19 20  6  5 19  9  1  9  5  1 14 20 19]\nStep 19: Action=244, Reward=7.40, Done=False, Win=False\nCurrent state letters: [14  1  1 14 15 11  1 20 19 20  6  9 19  5  1  9  5  1 14 20 19]\nStep 20: Action=244, Reward=19.40, Done=False, Win=False\nCurrent state letters: [14  1  1 14 15 11  1 20 19 20  6  5 19  9  1  9  5  1 14 20 19]\nStep 21: Action=244, Reward=7.40, Done=False, Win=False\nCurrent state letters: [14  1  1 14 15 11  1 20 19 20  6  9 19  5  1  9  5  1 14 20 19]\nStep 22: Action=244, Reward=19.40, Done=False, Win=False\nCurrent state letters: [14  1  1 14 15 11  1 20 19 20  6  5 19  9  1  9  5  1 14 20 19]\nStep 23: Action=244, Reward=7.40, Done=False, Win=False\nCurrent state letters: [14  1  1 14 15 11  1 20 19 20  6  9 19  5  1  9  5  1 14 20 19]\nStep 24: Action=244, Reward=19.40, Done=False, Win=False\nCurrent state letters: [14  1  1 14 15 11  1 20 19 20  6  5 19  9  1  9  5  1 14 20 19]\nStep 25: Action=244, Reward=7.40, Done=False, Win=False\nCurrent state letters: [14  1  1 14 15 11  1 20 19 20  6  9 19  5  1  9  5  1 14 20 19]\nStep 26: Action=244, Reward=19.40, Done=False, Win=False\nCurrent state letters: [14  1  1 14 15 11  1 20 19 20  6  5 19  9  1  9  5  1 14 20 19]\nStep 27: Action=244, Reward=7.40, Done=False, Win=False\nCurrent state letters: [14  1  1 14 15 11  1 20 19 20  6  9 19  5  1  9  5  1 14 20 19]\nStep 28: Action=244, Reward=19.40, Done=False, Win=False\nCurrent state letters: [14  1  1 14 15 11  1 20 19 20  6  5 19  9  1  9  5  1 14 20 19]\nStep 29: Action=244, Reward=7.40, Done=False, Win=False\nCurrent state letters: [14  1  1 14 15 11  1 20 19 20  6  9 19  5  1  9  5  1 14 20 19]\nStep 30: Action=244, Reward=19.40, Done=False, Win=False\nCurrent state letters: [14  1  1 14 15 11  1 20 19 20  6  5 19  9  1  9  5  1 14 20 19]\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"def manual_play(env, puzzle_index=0):\n    \"\"\"\n    Allows a human user to play by typing swap actions.\n    Displays the current letter grid after each move, showing green and yellow hints.\n\n    Args:\n        env: Waffle environment instance\n        puzzle_index: which puzzle to load and play\n    \"\"\"\n    env.reset(puzzle_index)\n    \n    def display_grid_with_colors(state_letters, features):\n        \"\"\"\n        Displays the waffle grid with colors encoded as:\n        - Green: uppercase letters (correct position)\n        - Yellow: lowercase letters (right letter wrong position)\n        - Grey: lowercase letters with dot prefix (letter does not belong here)\n        - Blank tiles are spaces\n        \"\"\"\n        position_to_grid = [\n            (0,0), (0,1), (0,2), (0,3), (0,4),\n            (1,0),   (1,2),   (1,4),\n            (2,0), (2,1), (2,2), (2,3), (2,4),\n            (3,0),   (3,2),   (3,4),\n            (4,0), (4,1), (4,2), (4,3), (4,4)\n        ]\n        \n        grid = [[' ' for _ in range(5)] for _ in range(5)]\n        print(state_letters)\n    \n        for flat_idx in range(len(state_letters)):\n            pos = position_to_grid[flat_idx]\n            if pos is not None:\n                r, c = pos\n                letter_code = state_letters[flat_idx]\n                if letter_code > 0:\n                    letter_char = chr(letter_code + ord('a') - 1)\n                    green = features[flat_idx, 0] >= 0.5\n                    yellow = features[flat_idx, 1] >= 0.5\n                    if green:\n                        display_char = letter_char.upper()\n                    elif yellow:\n                        display_char = letter_char.lower()\n                    else:\n                        # Grey letter shown with dot prefix\n                        display_char = '.' + letter_char.lower()\n                else:\n                    display_char = ' '\n                grid[r][c] = display_char\n    \n        print(\"Current Waffle Grid (Green=uppercase, Yellow=lowercase, Grey=.letter):\")\n        for row in grid:\n            print(' '.join(row))\n        print()\n    \n    print(f\"Starting manual play for puzzle {puzzle_index}\")\n    features = env._calculate_colors_and_embeddings(env.current_state)[..., :2]  # green, yellow only\n    display_grid_with_colors(env.get_current_state_letters(), features)\n    \n    while True:\n        valid_actions = env.get_valid_actions()\n        print(f\"Valid moves: {len(valid_actions)} possible swaps\")\n        print(\"Enter your swap action as two positions (0-20), separated by space (e.g., '3 15')\")\n        print(\"Positions map as per the 21 playable tiles, corners are not valid positions.\")\n        \n        user_input = input(\"Your swap (or 'quit' to exit): \")\n        if user_input.strip().lower() == 'quit':\n            print(\"Exiting manual play.\")\n            break\n        \n        try:\n            pos1, pos2 = map(int, user_input.strip().split())\n        except Exception:\n            print(\"Invalid input format. Please enter two integers separated by space.\")\n            continue\n        \n        if pos1 < 0 or pos1 >= 21 or pos2 < 0 or pos2 >= 21:\n            print(\"Positions must be integers in range 0-20.\")\n            continue\n        \n        # Convert swap to action index (order doesn’t matter, but model expects pos1*21 + pos2)\n        action = pos1 * 21 + pos2\n        if action not in valid_actions:\n            # Try reversed order in case it's the other way\n            reversed_action = pos2 * 21 + pos1\n            if reversed_action in valid_actions:\n                action = reversed_action\n            else:\n                print(\"Invalid action (not allowed to swap fixed/green positions or swapping the same). Try different positions.\")\n                continue\n        \n        _, reward, done, win = env.act(action)\n        print(f\"Performed swap {pos1} <-> {pos2}, Reward: {reward:.2f}\")\n        features = env._calculate_colors_and_embeddings(env.current_state)[..., :2]\n        display_grid_with_colors(env.get_current_state_letters(), features)\n        \n        if done:\n            if win:\n                print(\"Congratulations! You solved the puzzle!\")\n            else:\n                print(f\"Game over. Reached maximum moves ({env.max_moves}).\")\n            break\n\n\n# Example usage:\nfrom env import Waffle\nenv = Waffle(puzzles_file='/kaggle/input/waffle/waffles_shuffled_2k.jsonl', embeddings_file='/kaggle/input/waffle/letter_embeddings.npy')\nmanual_play(env, puzzle_index=1500)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-22T19:07:46.268445Z","iopub.status.idle":"2025-11-22T19:07:46.268651Z","shell.execute_reply.started":"2025-11-22T19:07:46.268554Z","shell.execute_reply":"2025-11-22T19:07:46.268562Z"}},"outputs":[],"execution_count":null}]}
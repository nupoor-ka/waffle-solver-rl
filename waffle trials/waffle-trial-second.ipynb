{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13743886,"sourceType":"datasetVersion","datasetId":8744658}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%writefile env.py\n\"\"\"\nWaffle Game Environment Definition Class - With Letter Embeddings\n\"\"\"\n\nimport numpy as np\nimport json\nfrom typing import Tuple, List, Optional\n\nclass Waffle(object):\n    def __init__(self, puzzles_file: str = \"waffles_shuffled.jsonl\", \n                 embeddings_file: str = \"letter_embeddings.npy\") -> None:\n        self.puzzles_file = puzzles_file\n        self.puzzles = self._load_puzzles()\n        self.embeddings = self._load_embeddings(embeddings_file)\n        self.grid_size = 21\n        self.current_puzzle = None\n        self.current_state = None\n        self.target_state = None\n        self.fixed_indices = None\n        \n        self.row_words = [\n            [0, 1, 2, 3, 4],\n            [8, 9, 10, 11, 12],\n            [16, 17, 18, 19, 20]\n        ]\n        self.col_words = [\n            [0, 5, 8, 13, 16],\n            [2, 6, 10, 14, 18],\n            [4, 7, 12, 15, 20]\n        ]\n        self.reset(0)\n\n    def _load_puzzles(self) -> List[dict]:\n        puzzles = []\n        with open(self.puzzles_file, \"r\") as f:\n            for line in f:\n                puzzles.append(json.loads(line))\n        return puzzles\n\n    def _load_embeddings(self, embeddings_file: str) -> np.ndarray:\n        try:\n            embeddings = np.load(embeddings_file)\n            print(f\"✓ Loaded embeddings: {embeddings.shape}\")\n            return embeddings.astype(np.float32)\n        except FileNotFoundError:\n            print(f\"⚠️ Creating random embeddings\")\n            return np.random.randn(27, 8).astype(np.float32)\n\n    def _string_to_array(self, flat21: str) -> np.ndarray:\n        return np.array([ord(c) - ord('a') + 1 for c in flat21], dtype=np.int32)\n\n    def _array_to_string(self, arr: np.ndarray) -> str:\n        return \"\".join([chr(int(x) + ord('a') - 1) for x in arr])\n\n    def _get_word_for_position(self, pos: int) -> Optional[List[int]]:\n        for word in self.row_words:\n            if pos in word:\n                return word\n        for word in self.col_words:\n            if pos in word:\n                return word\n        return None\n\n    def _calculate_colors_and_embeddings(self, state: np.ndarray) -> np.ndarray:\n        \"\"\"\n        Calculate green/yellow colors + letter embeddings for current state.\n        \n        Returns:\n            np.ndarray: Shape (21, 10) - [green, yellow, embedding_dim1, ..., embedding_dim8]\n        \"\"\"\n        features = np.zeros((21, 10), dtype=np.float32)\n        \n        for i in range(21):\n            current_letter = state[i]\n            target_letter = self.target_state[i]\n            \n            # Green: correct letter in correct position\n            if current_letter == target_letter:\n                features[i, 0] = 1.0  # Green channel\n            else:\n                # Find which word this position belongs to\n                word_positions = self._get_word_for_position(i)\n                if word_positions:\n                    target_word_letters = [self.target_state[p] for p in word_positions]\n                    # Yellow: letter exists in the same word but wrong position\n                    if current_letter in target_word_letters:\n                        features[i, 1] = 1.0  # Yellow channel\n                    # If neither green nor yellow, it's automatically grey (both 0)\n            \n            # Add letter embedding (8 dimensions)\n            # Letter encoding: a=1, b=2, ..., z=26, blank=0\n            letter_idx = int(current_letter) if current_letter <= 26 else 0\n            features[i, 2:10] = self.embeddings[letter_idx]\n        \n        return features\n\n    def _count_correct_positions(self, state: np.ndarray) -> int:\n        return np.sum(state == self.target_state)\n\n    # def reset(self) -> None:\n    #     self.current_puzzle = self.puzzles[np.random.randint(len(self.puzzles))]\n    #     self.target_state = self._string_to_array(self.current_puzzle[\"target_flat21\"])\n    #     self.current_state = self._string_to_array(self.current_puzzle[\"shuffled_flat21\"])\n    #     self.fixed_indices = set(self.current_puzzle[\"fixed_indices\"])\n    #     self.moves = 0\n    #     self.max_moves = 200\n    #     self.prev_correct_count = self._count_correct_positions(self.current_state)\n    #     self.prev_features = self._calculate_colors_and_embeddings(self.current_state)\n        \n    def reset(self, index: int) -> None:\n        self.current_puzzle = self.puzzles[index]\n        self.target_state = self._string_to_array(self.current_puzzle[\"target_flat21\"])\n        self.current_state = self._string_to_array(self.current_puzzle[\"shuffled_flat21\"])\n        self.fixed_indices = set(self.current_puzzle[\"fixed_indices\"])\n        self.moves = 0\n        self.max_moves = 200\n        self.prev_correct_count = self._count_correct_positions(self.current_state)\n        self.prev_features = self._calculate_colors_and_embeddings(self.current_state)\n\n    def _update_state(self, action: int) -> None:\n        pos1 = action // 21\n        pos2 = action % 21\n        if pos1 not in self.fixed_indices and pos2 not in self.fixed_indices:\n            temp = self.current_state[pos1]\n            self.current_state[pos1] = self.current_state[pos2]\n            self.current_state[pos2] = temp\n        self.moves += 1\n\n    def _get_reward(self, action: int) -> float:\n        pos1 = action // 21\n        pos2 = action % 21\n        reward = -0.1\n        \n        if np.array_equal(self.current_state, self.target_state):\n            return 1000.0 + reward\n        \n        current_correct = self._count_correct_positions(self.current_state)\n        correct_change = current_correct - self.prev_correct_count\n        current_features = self._calculate_colors_and_embeddings(self.current_state)\n        \n        if self.current_state[pos1] == self.current_state[pos2]:\n            reward += -1\n        \n        if (self.current_state[pos2] == self.target_state[pos1] or \n            self.current_state[pos1] == self.target_state[pos2]):\n            reward += -10\n        \n        if correct_change > 0:\n            reward += 1 * correct_change\n        elif correct_change < 0:\n            reward += -2 * abs(correct_change)\n        \n        # Check grey→yellow and yellow→grey transitions\n        for pos in [pos1, pos2]:\n            prev_green = self.prev_features[pos, 0]\n            prev_yellow = self.prev_features[pos, 1]\n            curr_green = current_features[pos, 0]\n            curr_yellow = current_features[pos, 1]\n            \n            # Grey → Yellow (neither green nor yellow before, yellow now)\n            if prev_green == 0 and prev_yellow == 0 and curr_yellow == 1.0:\n                reward += 1\n            # Yellow → Grey (yellow before, neither green nor yellow now)\n            elif prev_yellow == 1.0 and curr_green == 0 and curr_yellow == 0:\n                reward += -1\n        \n        self.prev_correct_count = current_correct\n        self.prev_features = current_features\n        return reward\n\n    def _is_over(self) -> bool:\n        solved = np.array_equal(self.current_state, self.target_state)\n        max_moves_reached = self.moves >= self.max_moves\n        return solved or max_moves_reached\n    \n    def _is_win(self) -> bool:\n        return np.array_equal(self.current_state, self.target_state)\n\n    def observe(self) -> np.ndarray:\n        \"\"\"\n        Return state with colors + embeddings.\n        \n        Returns:\n            np.ndarray: Shape (1, 21, 10) - 21 positions × 10 features (2 colors + 8 embedding dims)\n        \"\"\"\n        features = self._calculate_colors_and_embeddings(self.current_state)\n        return features.reshape(1, 21, 10)\n\n    def act(self, action: int) -> Tuple[np.ndarray, float, bool, bool]:\n        self._update_state(action)\n        reward = self._get_reward(action)\n        game_over = self._is_over()\n        game_win = self._is_win()\n        return self.observe(), reward, game_over, game_win\n\n    def get_valid_actions(self) -> List[int]:\n        valid_actions = []\n        movable_positions = [i for i in range(21) if i not in self.fixed_indices]\n        for i, pos1 in enumerate(movable_positions):\n            for pos2 in movable_positions[i+1:]:\n                action = pos1 * 21 + pos2\n                valid_actions.append(action)\n        return valid_actions\n\n    def get_num_actions(self) -> int:\n        return 21 * 21\n\n    def get_current_state_letters(self) -> np.ndarray:\n        return self.current_state","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-15T15:38:04.608646Z","iopub.execute_input":"2025-11-15T15:38:04.608926Z","iopub.status.idle":"2025-11-15T15:38:04.622098Z","shell.execute_reply.started":"2025-11-15T15:38:04.608904Z","shell.execute_reply":"2025-11-15T15:38:04.621492Z"}},"outputs":[{"name":"stdout","text":"Writing env.py\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"%%writefile experience_replay.py\n\"\"\"\nWaffle Game Experience Replay Class Definition - GPU Optimized\n\"\"\"\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom typing import Tuple\n\nclass ExperienceReplay(object):\n    def __init__(self, max_memory: int = 5000, discount: float = 0.95) -> None:\n        self.max_memory = max_memory\n        self.memory = list()\n        self.discount = discount\n\n    def add_experience(self, sars: list, game_over: bool) -> None:\n        self.memory.append([sars, game_over])\n        if len(self.memory) > self.max_memory:\n            del self.memory[0]\n\n    def get_qlearning_batch(\n        self, model: keras.Model, batch_size: int = 32\n    ) -> Tuple[np.ndarray, np.ndarray]:\n        memory_length = len(self.memory)\n        first_state = self.memory[0][0][0]\n        state_shape = first_state.shape[1:]\n        num_inputs = min(memory_length, batch_size)\n        ids = np.random.choice(memory_length, size=num_inputs, replace=False)\n        sars = list(zip(*[self.memory[id_][0] for id_ in ids]))\n        previous_states, action_ts, rewards, current_states = (\n            np.concatenate(e) if isinstance(e[0], np.ndarray) else np.stack(e)\n            for e in sars\n        )\n        game_over = np.stack([self.memory[id_][1] for id_ in ids])\n        \n        previous_states_tensor = tf.constant(previous_states, dtype=tf.float32)\n        current_states_tensor = tf.constant(current_states, dtype=tf.float32)\n        \n        targets = model(previous_states_tensor, training=False).numpy()\n        Q_sa = tf.reduce_max(model(current_states_tensor, training=False), axis=1).numpy()\n        targets[np.arange(num_inputs), action_ts] = (\n            rewards + self.discount * Q_sa * ~game_over\n        )\n        return previous_states, targets","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T15:38:04.758171Z","iopub.execute_input":"2025-11-15T15:38:04.758970Z","iopub.status.idle":"2025-11-15T15:38:04.763823Z","shell.execute_reply.started":"2025-11-15T15:38:04.758945Z","shell.execute_reply":"2025-11-15T15:38:04.763085Z"}},"outputs":[{"name":"stdout","text":"Writing experience_replay.py\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# Usually not needed, TensorFlow is pre-installed on Kaggle\nimport tensorflow as tf\nprint(f\"TensorFlow version: {tf.__version__}\")\nprint(f\"GPUs available: {len(tf.config.list_physical_devices('GPU'))}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T15:38:04.765037Z","iopub.execute_input":"2025-11-15T15:38:04.765243Z","iopub.status.idle":"2025-11-15T15:38:30.141658Z","shell.execute_reply.started":"2025-11-15T15:38:04.765228Z","shell.execute_reply":"2025-11-15T15:38:30.140852Z"}},"outputs":[{"name":"stderr","text":"2025-11-15 15:38:07.559048: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1763221087.951910      48 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1763221088.052024      48 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"name":"stdout","text":"TensorFlow version: 2.18.0\nGPUs available: 2\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import os\nimport json\nimport numpy as np\nimport logging\n\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Conv1D, Flatten, Dropout\nfrom tensorflow.keras.optimizers import Adam\n\nfrom experience_replay import ExperienceReplay\nfrom env import Waffle\n\n# GPU Configuration\nprint(\"=\"*60)\nprint(\"SYSTEM CHECK\")\nprint(\"=\"*60)\ngpus = tf.config.list_physical_devices('GPU')\nif gpus:\n    try:\n        for gpu in gpus:\n            tf.config.experimental.set_memory_growth(gpu, True)\n        print(f\"✓ {len(gpus)} GPU(s) configured\")\n    except RuntimeError as e:\n        print(f\"GPU warning (ignorable): {e}\")\nelse:\n    print(\"⚠️ No GPU\")\nprint(\"=\"*60)\n\n# Find dataset\ndataset_path = None\nembeddings_path = '/kaggle/input/letter_embeddings.npy'  # Or your uploaded path\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        if filename == 'waffles_shuffled.jsonl':\n            dataset_path = os.path.join(dirname, filename)\n            print(f\"\\n✓ Found dataset: {dataset_path}\")\n        if filename == 'letter_embeddings.npy':\n            embeddings_path = os.path.join(dirname, filename)\n            print(f\"✓ Found embeddings: {embeddings_path}\")\n\nif not dataset_path:\n    raise FileNotFoundError(\"waffles_shuffled.jsonl not found\")\n\n# Model Definition - CHANGED INPUT SHAPE\ndef define_model(input_shape, hidden_size, num_actions, \n                learning_rate=0.001, conv_filters=64, loss=\"mse\"):\n    policy = tf.keras.mixed_precision.Policy('mixed_float16')\n    tf.keras.mixed_precision.set_global_policy(policy)\n    \n    model = Sequential()\n    model.add(Conv1D(conv_filters, kernel_size=3, activation='relu', \n                     padding='same', input_shape=input_shape, dtype='float32'))\n    model.add(Conv1D(conv_filters, kernel_size=3, activation='relu', padding='same'))\n    model.add(Conv1D(conv_filters * 2, kernel_size=5, activation='relu', padding='same'))\n    model.add(Flatten())\n    model.add(Dense(hidden_size, activation='relu'))\n    model.add(Dropout(0.2))\n    model.add(Dense(hidden_size // 2, activation='relu'))\n    model.add(Dropout(0.2))\n    model.add(Dense(num_actions, dtype='float32'))\n    model.compile(Adam(learning_rate=learning_rate), loss)\n    return model\n\n# Training Function\ndef train_model(model, env, epochs, experience_replay, \n               epsilon_start, epsilon_end, epsilon_decay, batch_size):\n    logging.info(\"Starting training...\")\n    win_count = 0\n    total_rewards = []\n    epsilon = epsilon_start\n    num_actions = env.get_num_actions()\n    num_puzzles = len(env.puzzles)\n    \n    @tf.function\n    def predict_q(state):\n        return model(state, training=False)\n    \n    for epoch in range(epochs):\n        loss = 0.0\n        for i in range(num_puzzles):\n            env.reset(i)\n            current_state = env.observe()\n            game_over = False\n            episode_reward = 0\n            step_count = 0\n            \n            while not game_over:\n                previous_state = current_state\n                \n                if np.random.rand() <= epsilon:\n                    valid_actions = env.get_valid_actions()\n                    action = np.random.choice(valid_actions) if valid_actions else np.random.randint(0, num_actions)\n                else:\n                    q = predict_q(tf.constant(previous_state, dtype=tf.float32))\n                    action = int(tf.argmax(q[0]).numpy())\n                \n                current_state, reward, game_over, game_win = env.act(action)\n                episode_reward += reward\n                step_count += 1\n                \n                if game_win:\n                    win_count += 1\n                \n                experience_replay.add_experience(\n                    [previous_state, int(action), reward, current_state], game_over\n                )\n                \n                if len(experience_replay.memory) >= batch_size and step_count % 5 == 0:\n                    inputs, targets = experience_replay.get_qlearning_batch(model, batch_size=batch_size)\n                    loss += model.train_on_batch(inputs, targets)\n\n            total_rewards.append(episode_reward)\n        \n        epsilon = max(epsilon_end, epsilon * epsilon_decay)\n        \n        if (epoch + 1) % 10 == 0 or epoch < 10:\n            avg_reward = np.mean(total_rewards[-100:]) if len(total_rewards) >= 100 else np.mean(total_rewards)\n            win_rate = win_count / (epoch + 1) * 100\n            print(f\"Ep {epoch+1:4d}/{epochs} | Loss: {loss:8.4f} | ε: {epsilon:.4f} | \"\n                  f\"Wins: {win_count:3d} ({win_rate:5.1f}%) | AvgR: {avg_reward:7.2f}\")\n        \n        if (epoch + 1) % 100 == 0:\n            print(f\"\\n{'='*70}\")\n            print(f\"CHECKPOINT - Epoch {epoch+1}/{epochs}\")\n            print(f\"Wins: {win_count}/{epoch+1} ({win_rate:.1f}%)\")\n            print(f\"Avg reward (last 100): {np.mean(total_rewards[-100:]):.2f}\")\n            print(f\"{'='*70}\\n\")\n    \n    print(f\"\\nTraining complete! Win rate: {win_count}/{epochs} ({win_count/epochs*100:.1f}%)\")\n    return model\n\n# Hyperparameters\nEPOCHS = 200\nEPSILON_START = 1.0\nEPSILON_END = 0.01\nEPSILON_DECAY = 0.995\nMAX_MEMORY = 10000\nHIDDEN_SIZE = 512\nCONV_FILTERS = 64\nBATCH_SIZE = 128\nDISCOUNT = 0.99\nLEARNING_RATE = 0.0005\n\n# Initialize Environment with embeddings\nenv = Waffle(puzzles_file=dataset_path, embeddings_file=embeddings_path)\nnum_actions = env.get_num_actions()\ninput_shape = (21, 10)  # CHANGED: 21 positions × 10 features (2 colors + 8 embeddings)\n\nprint(f\"\\n{'='*70}\")\nprint(f\"ENVIRONMENT SETUP\")\nprint(f\"{'='*70}\")\nprint(f\"Puzzles loaded:      {len(env.puzzles)}\")\nprint(f\"Action space:        {num_actions}\")\nprint(f\"Input shape:         {input_shape} (21 pos × [2 colors + 8 embed])\")\nprint(f\"{'='*70}\\n\")\n\n# Build Model\nmodel = define_model(\n    input_shape=input_shape,\n    hidden_size=HIDDEN_SIZE,\n    num_actions=num_actions,\n    learning_rate=LEARNING_RATE,\n    conv_filters=CONV_FILTERS\n)\n\nmodel.summary()\n\n# GPU Test\ntest_input = tf.random.normal([1, 21, 10])\ntest_output = model(test_input, training=False)\nprint(f\"\\n✓ GPU test passed\")\nprint(f\"  Input: {test_input.device}\")\nprint(f\"  Output: {test_output.device}\\n\")\n\n# Train\nexp_replay = ExperienceReplay(max_memory=MAX_MEMORY, discount=DISCOUNT)\nlogging.basicConfig(level=logging.INFO)\n\ntrained_model = train_model(\n    model=model,\n    env=env,\n    epochs=EPOCHS,\n    experience_replay=exp_replay,\n    epsilon_start=EPSILON_START,\n    epsilon_end=EPSILON_END,\n    epsilon_decay=EPSILON_DECAY,\n    batch_size=BATCH_SIZE\n)\n\n# Save Model - FIXED\noutput_dir = '/kaggle/working/model'\nos.makedirs(output_dir, exist_ok=True)\n\n# FIX: Use .weights.h5 extension\ntrained_model.save_weights(f'{output_dir}/model.weights.h5', overwrite=True)\nwith open(f'{output_dir}/model.json', 'w') as f:\n    json.dump(trained_model.to_json(), f)\n\nprint(f\"\\n✓ Model saved to {output_dir}/\")\nprint(f\"  - model.weights.h5\")\nprint(f\"  - model.json\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T15:38:30.142542Z","iopub.execute_input":"2025-11-15T15:38:30.142951Z","execution_failed":"2025-11-15T18:02:19.733Z"}},"outputs":[{"name":"stdout","text":"============================================================\nSYSTEM CHECK\n============================================================\n✓ 2 GPU(s) configured\n============================================================\n\n✓ Found dataset: /kaggle/input/waffle/waffles_shuffled.jsonl\n✓ Found embeddings: /kaggle/input/waffle/letter_embeddings.npy\n✓ Loaded embeddings: (27, 8)\n\n======================================================================\nENVIRONMENT SETUP\n======================================================================\nPuzzles loaded:      100\nAction space:        441\nInput shape:         (21, 10) (21 pos × [2 colors + 8 embed])\n======================================================================\n\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\nI0000 00:00:1763221110.560302      48 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13942 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\nI0000 00:00:1763221110.561085      48 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 13942 MB memory:  -> device: 1, name: Tesla T4, pci bus id: 0000:00:05.0, compute capability: 7.5\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"sequential\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ conv1d (\u001b[38;5;33mConv1D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m21\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │         \u001b[38;5;34m1,984\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv1d_1 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m21\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │        \u001b[38;5;34m12,352\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv1d_2 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m21\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │        \u001b[38;5;34m41,088\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2688\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │     \u001b[38;5;34m1,376,768\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │       \u001b[38;5;34m131,328\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m441\u001b[0m)            │       \u001b[38;5;34m113,337\u001b[0m │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ conv1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">21</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,984</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv1d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">21</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">12,352</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv1d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">21</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">41,088</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2688</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,376,768</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">441</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">113,337</span> │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,676,857\u001b[0m (6.40 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,676,857</span> (6.40 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,676,857\u001b[0m (6.40 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,676,857</span> (6.40 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}},{"name":"stderr","text":"I0000 00:00:1763221112.495289      48 cuda_dnn.cc:529] Loaded cuDNN version 90300\nINFO:root:Starting training...\n","output_type":"stream"},{"name":"stdout","text":"\n✓ GPU test passed\n  Input: /job:localhost/replica:0/task:0/device:GPU:0\n  Output: /job:localhost/replica:0/task:0/device:GPU:0\n\n","output_type":"stream"},{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1763221116.325126      48 service.cc:148] XLA service 0x564cc660 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\nI0000 00:00:1763221116.326839      48 service.cc:156]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\nI0000 00:00:1763221116.326864      48 service.cc:156]   StreamExecutor device (1): Tesla T4, Compute Capability 7.5\nI0000 00:00:1763221120.301907      48 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","output_type":"stream"},{"name":"stdout","text":"Ep    1/200 | Loss: 367116.5978 | ε: 0.9950 | Wins:   0 (  0.0%) | AvgR: -541.08\nEp    2/200 | Loss: 38292.4113 | ε: 0.9900 | Wins:   0 (  0.0%) | AvgR: -530.81\nEp    3/200 | Loss: 22472.1784 | ε: 0.9851 | Wins:   0 (  0.0%) | AvgR: -538.78\nEp    4/200 | Loss: 16012.6617 | ε: 0.9801 | Wins:   0 (  0.0%) | AvgR: -522.67\nEp    5/200 | Loss: 12481.0264 | ε: 0.9752 | Wins:   0 (  0.0%) | AvgR: -540.10\nEp    6/200 | Loss: 10240.6189 | ε: 0.9704 | Wins:   0 (  0.0%) | AvgR: -539.55\nEp    7/200 | Loss: 8694.9939 | ε: 0.9655 | Wins:   0 (  0.0%) | AvgR: -537.24\nEp    8/200 | Loss: 7563.5463 | ε: 0.9607 | Wins:   0 (  0.0%) | AvgR: -523.69\nEp    9/200 | Loss: 6698.8280 | ε: 0.9559 | Wins:   0 (  0.0%) | AvgR: -523.02\nEp   10/200 | Loss: 6016.9142 | ε: 0.9511 | Wins:   0 (  0.0%) | AvgR: -525.09\nEp   20/200 | Loss: 3053.5714 | ε: 0.9046 | Wins:   0 (  0.0%) | AvgR: -519.45\nEp   30/200 | Loss: 2098.0295 | ε: 0.8604 | Wins:   0 (  0.0%) | AvgR: -515.55\nEp   40/200 | Loss: 1626.9329 | ε: 0.8183 | Wins:   0 (  0.0%) | AvgR: -512.96\nEp   50/200 | Loss: 1345.5999 | ε: 0.7783 | Wins:   0 (  0.0%) | AvgR: -493.11\nEp   60/200 | Loss: 1158.6485 | ε: 0.7403 | Wins:   0 (  0.0%) | AvgR: -498.63\nEp   70/200 | Loss: 1024.7221 | ε: 0.7041 | Wins:   0 (  0.0%) | AvgR: -478.75\n","output_type":"stream"}],"execution_count":null}]}
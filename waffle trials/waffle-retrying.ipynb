{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13832040,"sourceType":"datasetVersion","datasetId":8809238}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%writefile env.py\n\n# CREATING WAFFLE ENVIRONMENT\n\nimport numpy as np\nimport json\nfrom typing import Tuple, List, Optional, Set\n\nclass Waffle(object):\n    def __init__(self, puzzles_file: str, \n                 embeddings_file: str,\n                 vocab_file: str = \"/kaggle/input/waffler/words.txt\") -> None:\n        self.puzzles_file = puzzles_file\n        self.puzzles = self._load_puzzles()\n        self.embeddings = self._load_embeddings(embeddings_file)\n        self.grid_size = 21\n        self.current_puzzle = None\n        self.current_state = None\n        self.target_state = None\n        self.fixed_indices = None\n        \n        self.row_words = [\n            [0, 1, 2, 3, 4],\n            [8, 9,10,11,12],\n            [16,17,18,19,20]\n        ]\n        self.col_words = [\n            [0, 5, 8,13,16],\n            [2, 6,10,14,18],\n            [4, 7,12,15,20]\n        ]\n\n        self.vocab = self._load_vocab(vocab_file)\n\n        self.reset(0)\n\n    def _load_puzzles(self) -> List[dict]:\n        puzzles = []\n        with open(self.puzzles_file, \"r\") as f:\n            for line in f:\n                puzzles.append(json.loads(line))\n        return puzzles\n\n    def _load_vocab(self, vocab_file: str) -> Set[str]:\n        vocab = set()\n        with open(vocab_file, 'r') as f:\n            for line in f:\n                word = line.strip().lower()\n                if len(word) == 5:\n                    vocab.add(word)\n        print(f\"Loaded vocabulary with {len(vocab)} words\")\n        return vocab\n\n    def _load_embeddings(self, embeddings_file: str) -> np.ndarray:\n        try:\n            embeddings = np.load(embeddings_file)\n            print(f\"Loaded embeddings: {embeddings.shape}\")\n            return embeddings.astype(np.float32)\n        except FileNotFoundError:\n            print(f\"Creating random embeddings\")\n            return np.random.randn(27, 8).astype(np.float32)\n\n    def _string_to_array(self, flat21: str) -> np.ndarray:\n        return np.array([ord(c) - ord('a') + 1 for c in flat21], dtype=np.int32)\n\n    def _array_to_string(self, arr: np.ndarray) -> str:\n        return \"\".join([chr(int(x) + ord('a') - 1) for x in arr])\n\n    def reset(self, index: int) -> None:\n        self.current_puzzle = self.puzzles[index]\n        self.target_state = self._string_to_array(self.current_puzzle[\"target_flat21\"])\n        self.current_state = self._string_to_array(self.current_puzzle[\"shuffled_flat21\"])\n        self.fixed_indices = set(self.current_puzzle[\"fixed_indices\"])\n        self.moves = 0\n        self.max_moves = 50\n        self.state_history = [self.current_state.copy()]\n        self.prev_word_count = self._count_words(self.current_state)\n\n    def _update_state(self, action: int) -> None:\n        pos1 = action // 21\n        pos2 = action % 21\n        temp = self.current_state[pos1]\n        self.current_state[pos1] = self.current_state[pos2]\n        self.current_state[pos2] = temp\n        self.moves += 1\n\n    def _count_words(self, state: np.ndarray) -> int:\n        letters = [chr(c + ord('a') - 1) for c in state]\n        count = 0\n        for indices in self.row_words + self.col_words:\n            word_str = ''.join([letters[i] for i in indices])\n            if word_str in self.vocab:\n                count += 1\n        return count\n\n    def _is_over(self) -> bool:\n        return self._is_win() or self.moves >= self.max_moves\n    \n    def _is_win(self) -> bool:\n        return np.array_equal(self.current_state, self.target_state)\n\n    def observe(self) -> np.ndarray:\n        # Just for simplicity, use a zero-mean/normalized \"feature cube\" (no change from before here)\n        features = np.zeros((21, 10), dtype=np.float32)\n        for i in range(21):\n            current_letter = self.current_state[i]\n            target_letter = self.target_state[i]\n            features[i, 0] = float(current_letter == target_letter)\n            letter_idx = int(current_letter) - 1 if current_letter >= 1 else 26\n            features[i, 2:10] = self.embeddings[letter_idx]\n        grid = np.zeros((5, 5, 10), dtype=np.float32)\n        blank_embedding = self.embeddings[26]\n        blank_feature_vector = np.zeros(10, dtype=np.float32)\n        blank_feature_vector[2:10] = blank_embedding\n        blank_positions = [(1,1), (1,3), (3,1), (3,3)]\n        for r, c in blank_positions:\n            grid[r, c, :] = blank_feature_vector\n        position_to_grid = [\n            (0,0), (0,1), (0,2), (0,3), (0,4),\n            (1,0), (1,2), (1,4),\n            (2,0), (2,1), (2,2), (2,3), (2,4),\n            (3,0), (3,2), (3,4),\n            (4,0), (4,1), (4,2), (4,3), (4,4)\n        ]\n        for flat_idx in range(21):\n            r, c = position_to_grid[flat_idx]\n            grid[r, c, :] = features[flat_idx]\n        return grid.reshape(1, 5, 5, 10)\n\n    def get_valid_actions(self) -> List[int]:\n        positions = [pos for pos in range(21) if pos not in self.fixed_indices]\n        valid_actions = []\n        for i, pos1 in enumerate(positions):\n            for pos2 in positions[i+1:]:\n                action = pos1 * 21 + pos2\n                valid_actions.append(action)\n        return valid_actions\n\n    def get_num_actions(self) -> int:\n        return 21 * 21\n\n    # ---- PRIMARY: Revised reward system ----\n    def _get_reward(self, action: int) -> float:\n        reward = 0.0\n        win = np.array_equal(self.current_state, self.target_state)\n        if win:\n            reward = 1.0\n        elif self.moves >= self.max_moves:\n            reward = -1.0\n        else:\n            # Reward only net gain in valid word count, penalize loss\n            word_count = self._count_words(self.current_state)\n            delta_word = word_count - self.prev_word_count\n            if delta_word > 0:\n                reward += 0.2 * delta_word\n            elif delta_word < 0:\n                reward -= 0.2 * abs(delta_word)\n            reward -= 0.01\n            # Heavier penalty for \"undoing\" (flip-flop) swaps\n            if len(self.state_history) > 2 and np.array_equal(self.current_state, self.state_history[-2]):\n                reward -= 0.05\n        self.prev_word_count = self._count_words(self.current_state)\n        self.state_history.append(self.current_state.copy())\n        if len(self.state_history) > 10:\n            self.state_history.pop(0)\n        return reward\n\n    def act(self, action: int) -> Tuple[np.ndarray, float, bool, bool]:\n        self._update_state(action)\n        reward = self._get_reward(action)\n        game_over = self._is_over()\n        game_win = self._is_win()\n        return self.observe(), reward, game_over, game_win\n\n    def get_current_state_letters(self) -> np.ndarray:\n        return self.current_state","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-22T19:59:47.036772Z","iopub.execute_input":"2025-11-22T19:59:47.037244Z","iopub.status.idle":"2025-11-22T19:59:47.044434Z","shell.execute_reply.started":"2025-11-22T19:59:47.037223Z","shell.execute_reply":"2025-11-22T19:59:47.043668Z"}},"outputs":[{"name":"stdout","text":"Writing env.py\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"%%writefile experience_replay.py\n\n# WRITING EXPERIENCE REPLAY LOGIC\n\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\n\nclass ExperienceReplay(object):\n    def __init__(self, max_memory: int = 10000, discount: float = 0.99) -> None:\n        self.max_memory = max_memory\n        self.memory = []\n        self.discount = discount\n\n    def add_experience(self, sars: list, game_over: bool) -> None:\n        self.memory.append([sars, game_over])\n        if len(self.memory) > self.max_memory:\n            del self.memory[0]\n\n    def get_qlearning_batch(self, model, target_model, batch_size: int = 128):\n        memory_length = len(self.memory)\n        num_inputs = min(memory_length, batch_size)\n        ids = np.random.choice(memory_length, size=num_inputs, replace=False)\n        sars = list(zip(*[self.memory[id_][0] for id_ in ids]))\n        previous_states, action_ts, rewards, current_states = (\n            np.concatenate(e) if isinstance(e[0], np.ndarray) else np.stack(e)\n            for e in sars\n        )\n        game_over = np.array([self.memory[id_][1] for id_ in ids], dtype=bool)\n        previous_states_tensor = tf.constant(previous_states, dtype=tf.float32)\n        current_states_tensor = tf.constant(current_states, dtype=tf.float32)\n        targets = model(previous_states_tensor, training=False).numpy()\n        next_q_main = model(current_states_tensor, training=False).numpy()\n        best_actions = np.argmax(next_q_main, axis=1)\n        next_q_target = target_model(current_states_tensor, training=False).numpy()\n        Q_sa = next_q_target[np.arange(num_inputs), best_actions]\n        targets[np.arange(num_inputs), action_ts] = (\n            rewards + self.discount * Q_sa * (~game_over)\n        )\n        return previous_states, targets\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-22T19:59:47.045387Z","iopub.execute_input":"2025-11-22T19:59:47.045684Z","iopub.status.idle":"2025-11-22T19:59:47.073109Z","shell.execute_reply.started":"2025-11-22T19:59:47.045657Z","shell.execute_reply":"2025-11-22T19:59:47.072213Z"}},"outputs":[{"name":"stdout","text":"Writing experience_replay.py\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import tensorflow as tf\nprint(\"Num GPUs Available:\", len(tf.config.list_physical_devices('GPU')))\nprint(\"TensorFlow running on GPU:\", tf.test.is_gpu_available())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-22T19:59:29.880626Z","iopub.execute_input":"2025-11-22T19:59:29.880957Z","iopub.status.idle":"2025-11-22T19:59:47.035447Z","shell.execute_reply.started":"2025-11-22T19:59:29.880933Z","shell.execute_reply":"2025-11-22T19:59:47.034795Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"name":"stdout","text":"Num GPUs Available: 2\nWARNING:tensorflow:From /tmp/ipykernel_48/1247457372.py:3: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse `tf.config.list_physical_devices('GPU')` instead.\nTensorFlow running on GPU: True\n","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1763841587.030596      48 gpu_device.cc:2022] Created device /device:GPU:0 with 13942 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\nI0000 00:00:1763841587.031342      48 gpu_device.cc:2022] Created device /device:GPU:1 with 13942 MB memory:  -> device: 1, name: Tesla T4, pci bus id: 0000:00:05.0, compute capability: 7.5\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import os\nimport json\nimport numpy as np\nimport logging\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Conv2D, Flatten\nfrom tensorflow.keras.optimizers import Adam\n\nfrom experience_replay import ExperienceReplay\nfrom env import Waffle\n\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n\ndef define_model(input_shape, hidden_size, num_actions, learning_rate=0.0001):\n    # Robust Adam with gradient clipping\n    optimizer = Adam(learning_rate=learning_rate, clipnorm=1.0)\n    model = Sequential()\n    model.add(Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=input_shape, dtype='float32'))\n    model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n    model.add(Flatten())\n    model.add(Dense(hidden_size, activation='relu'))\n    model.add(Dense(hidden_size//2, activation='relu'))\n    model.add(Dense(num_actions, dtype='float32'))\n    model.compile(optimizer, loss='huber')  # Huber loss for DQN stability\n    return model\n\n# == Find dataset == #\ndataset_path = None\nembeddings_path = '/kaggle/input/letter_embeddings.npy'\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        if filename == 'waffles_shuffled_2k.jsonl':\n            dataset_path = os.path.join(dirname, filename)\n        if filename == 'letter_embeddings.npy':\n            embeddings_path = os.path.join(dirname, filename)\nif not dataset_path:\n    raise FileNotFoundError(\"waffles_shuffled_2k.jsonl not found\")\n\n# == Hyperparameters == #\nEPOCHS = 40\nEPSILON_START = 1.0\nEPSILON_END = 0.01\nTARGET_EPSILON_EPOCH = 100\nEPSILON_DECAY = (EPSILON_END / EPSILON_START) ** (1 / TARGET_EPSILON_EPOCH)\nMAX_MEMORY = 10000\nHIDDEN_SIZE = 256\nBATCH_SIZE = 128\nDISCOUNT = 0.99\nLEARNING_RATE = 0.0001       # Lowered!\nPUZZLES_PER_EPOCH = 400\nTARGET_UPDATE_FREQUENCY = 1000\n\n# == Environment & Models == #\nenv = Waffle(puzzles_file=dataset_path, embeddings_file=embeddings_path)\nnum_actions = env.get_num_actions()\ninput_shape = (5, 5, 10)\n\nmodel = define_model(input_shape, HIDDEN_SIZE, num_actions, LEARNING_RATE)\ntarget_model = define_model(input_shape, HIDDEN_SIZE, num_actions, LEARNING_RATE)\ntarget_model.set_weights(model.get_weights())\n\nexp_replay = ExperienceReplay(max_memory=MAX_MEMORY, discount=DISCOUNT)\nlogging.basicConfig(level=logging.INFO)\n\ndef sample_puzzles(epoch, env, epochs, puzzles_per_epoch):\n    initial_correctness_list = []\n    for idx, p in enumerate(env.puzzles):\n        state = env._string_to_array(p[\"shuffled_flat21\"])\n        target = env._string_to_array(p[\"target_flat21\"])\n        correct = np.sum(state == target)\n        initial_correctness_list.append((idx, correct))\n    bins = {\n        'easy':   [idx for idx, c in initial_correctness_list if c >= 9],\n        'medium': [idx for idx, c in initial_correctness_list if 7 <= c < 9],\n        'hard':   [idx for idx, c in initial_correctness_list if 5 <= c < 7],\n        'harder':[idx for idx, c in initial_correctness_list if c < 5]\n    }\n    fraction = epoch / epochs\n    if fraction < 0.25:\n        return np.random.choice(bins['easy'], size=puzzles_per_epoch, replace=True)\n    elif fraction < 0.50:\n        mixed = bins['easy'] + bins['medium']\n        return np.random.choice(mixed, size=puzzles_per_epoch, replace=True)\n    elif fraction < 0.75:\n        mixed = bins['medium'] + bins['hard']\n        return np.random.choice(mixed, size=puzzles_per_epoch, replace=True)\n    else:\n        mixed = bins['hard'] + bins['harder']\n        return np.random.choice(mixed, size=puzzles_per_epoch, replace=True)\n\ndef train_model(model, target_model, env, epochs, experience_replay,\n               epsilon_start, epsilon_end, epsilon_decay, batch_size,\n               puzzles_per_epoch, target_update_freq):\n    win_count = 0\n    total_rewards = []\n    epsilon = epsilon_start\n    num_actions = env.get_num_actions()\n    training_steps = 0\n    for epoch in range(epochs):\n        loss = 0.0\n        epoch_wins = 0\n        avg_moves = []\n        correct_letters = []\n        puzzle_indices = sample_puzzles(epoch, env, epochs, puzzles_per_epoch)\n        for puzzle_idx in puzzle_indices:\n            env.reset(puzzle_idx)\n            current_state = env.observe()\n            game_over = False\n            episode_reward = 0\n            step_count = 0\n\n            while not game_over:\n                valid_actions = env.get_valid_actions()\n                if np.random.rand() <= epsilon:\n                    action = np.random.choice(valid_actions)\n                else:\n                    q = model.predict(current_state, verbose=0)[0]\n                    masked_q = np.full(num_actions, -np.inf)\n                    masked_q[valid_actions] = q[valid_actions]\n                    action = int(np.argmax(masked_q))\n                next_state, reward, game_over, game_win = env.act(action)\n                episode_reward += reward\n                step_count += 1\n                experience_replay.add_experience(\n                    [current_state, int(action), reward, next_state],\n                    game_over\n                )\n                current_state = next_state\n\n                if len(experience_replay.memory) >= batch_size and step_count % 5 == 0:\n                    inputs, targets = experience_replay.get_qlearning_batch(\n                        model, target_model, batch_size=batch_size)\n                    loss += model.train_on_batch(inputs, targets)\n                    training_steps += 1\n                    if training_steps % target_update_freq == 0:\n                        target_model.set_weights(model.get_weights())\n                        print(f\"  [Target network updated at step {training_steps}]\")\n            total_rewards.append(episode_reward)\n            if game_win:\n                epoch_wins += 1\n            avg_moves.append(step_count)\n            num_correct = np.sum(env.current_state == env.target_state)\n            correct_letters.append(num_correct)\n        win_count += epoch_wins\n        avg_moves_epoch = np.mean(avg_moves)\n        avg_correct_epoch = np.mean(correct_letters)\n        epsilon = max(epsilon_end, epsilon * epsilon_decay)\n        recent_window = min(400, len(total_rewards))\n        avg_reward = np.mean(total_rewards[-recent_window:])\n        win_percent = epoch_wins / puzzles_per_epoch * 100\n        print(f\"Ep {epoch+1}/{epochs} | Loss: {loss:8.4f} | ε: {epsilon:.4f} | \"\n              f\"Wins in epoch: {epoch_wins} ({win_percent:5.2f}%) | \"\n              f\"AvgMoves: {avg_moves_epoch:.1f} | \"\n              f\"AvgCorrectAtEnd: {avg_correct_epoch:.1f}/21 | \"\n              f\"AvgR: {avg_reward:7.2f}\")\n\ntrained_model = train_model(\n    model=model,\n    target_model=target_model,\n    env=env,\n    epochs=EPOCHS,\n    experience_replay=exp_replay,\n    epsilon_start=EPSILON_START,\n    epsilon_end=EPSILON_END,\n    epsilon_decay=EPSILON_DECAY,\n    batch_size=BATCH_SIZE,\n    puzzles_per_epoch=PUZZLES_PER_EPOCH,\n    target_update_freq=TARGET_UPDATE_FREQUENCY\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-22T20:12:28.550522Z","iopub.execute_input":"2025-11-22T20:12:28.550855Z","execution_failed":"2025-11-23T04:36:43.410Z"}},"outputs":[{"name":"stdout","text":"Loaded embeddings: (27, 8)\nLoaded vocabulary with 5757 words\n  [Target network updated at step 1000]\n  [Target network updated at step 2000]\n  [Target network updated at step 3000]\nEp 1/40 | Loss: 318.6378 | ε: 0.9550 | Wins in epoch: 0 ( 0.00%) | AvgMoves: 50.0 | AvgCorrectAtEnd: 7.2/21 | AvgR:   -1.55\n  [Target network updated at step 4000]\n  [Target network updated at step 5000]\n  [Target network updated at step 6000]\n  [Target network updated at step 7000]\nEp 2/40 | Loss: 1184.1132 | ε: 0.9120 | Wins in epoch: 0 ( 0.00%) | AvgMoves: 50.0 | AvgCorrectAtEnd: 7.1/21 | AvgR:   -1.55\n  [Target network updated at step 8000]\n  [Target network updated at step 9000]\n  [Target network updated at step 10000]\n  [Target network updated at step 11000]\nEp 3/40 | Loss: 4579.7545 | ε: 0.8710 | Wins in epoch: 0 ( 0.00%) | AvgMoves: 50.0 | AvgCorrectAtEnd: 7.2/21 | AvgR:   -1.56\n  [Target network updated at step 12000]\n  [Target network updated at step 13000]\n  [Target network updated at step 14000]\n  [Target network updated at step 15000]\nEp 4/40 | Loss: 13811.6399 | ε: 0.8318 | Wins in epoch: 0 ( 0.00%) | AvgMoves: 50.0 | AvgCorrectAtEnd: 7.4/21 | AvgR:   -1.57\n  [Target network updated at step 16000]\n  [Target network updated at step 17000]\n  [Target network updated at step 18000]\n  [Target network updated at step 19000]\nEp 5/40 | Loss: 33389.2759 | ε: 0.7943 | Wins in epoch: 0 ( 0.00%) | AvgMoves: 50.0 | AvgCorrectAtEnd: 7.3/21 | AvgR:   -1.61\n  [Target network updated at step 20000]\n  [Target network updated at step 21000]\n  [Target network updated at step 22000]\n  [Target network updated at step 23000]\nEp 6/40 | Loss: 69013.6766 | ε: 0.7586 | Wins in epoch: 0 ( 0.00%) | AvgMoves: 50.0 | AvgCorrectAtEnd: 7.1/21 | AvgR:   -1.63\n  [Target network updated at step 24000]\n  [Target network updated at step 25000]\n  [Target network updated at step 26000]\n  [Target network updated at step 27000]\nEp 7/40 | Loss: 104735.0090 | ε: 0.7244 | Wins in epoch: 0 ( 0.00%) | AvgMoves: 50.0 | AvgCorrectAtEnd: 7.1/21 | AvgR:   -1.67\n  [Target network updated at step 28000]\n  [Target network updated at step 29000]\n  [Target network updated at step 30000]\n","output_type":"stream"}],"execution_count":null}]}
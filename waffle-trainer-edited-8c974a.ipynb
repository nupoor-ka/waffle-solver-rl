{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-11-22T20:17:15.496974Z",
     "iopub.status.busy": "2025-11-22T20:17:15.496706Z",
     "iopub.status.idle": "2025-11-22T20:17:15.509010Z",
     "shell.execute_reply": "2025-11-22T20:17:15.508436Z",
     "shell.execute_reply.started": "2025-11-22T20:17:15.496942Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing env.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile env.py\n",
    "\"\"\"\n",
    "Waffle Environment - DENSE REWARD SHAPING for RL\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import json\n",
    "from typing import Tuple, List, Optional\n",
    "\n",
    "class Waffle(object):\n",
    "    def __init__(self, puzzles_file: str = \"/kaggle/input/waffle/waffles_shuffled_2k.jsonl\", \n",
    "                 embeddings_file: str = \"/kaggle/input/waffle/letter_embeddings.npy\",\n",
    "                 vocab_file: str = \"/kaggle/input/waffle/words.txt\") -> None:\n",
    "        self.puzzles_file = puzzles_file\n",
    "        self.puzzles = self._load_puzzles()\n",
    "        self.embeddings = self._load_embeddings(embeddings_file)\n",
    "        self.grid_size = 21\n",
    "        self.current_puzzle = None\n",
    "        self.current_state = None\n",
    "        self.target_state = None\n",
    "        self.fixed_indices = None\n",
    "        \n",
    "        self.row_words = [\n",
    "            [0, 1, 2, 3, 4],\n",
    "            [8, 9, 10, 11, 12],\n",
    "            [16, 17, 18, 19, 20]\n",
    "        ]\n",
    "        self.col_words = [\n",
    "            [0, 5, 8, 13, 16],\n",
    "            [2, 6, 10, 14, 18],\n",
    "            [4, 7, 12, 15, 20]\n",
    "        ]\n",
    "\n",
    "        self.vocab = self._load_vocab(vocab_file)\n",
    "        self.reset(0)\n",
    "\n",
    "    def _load_puzzles(self) -> List[dict]:\n",
    "        puzzles = []\n",
    "        with open(self.puzzles_file, \"r\") as f:\n",
    "            for line in f:\n",
    "                puzzles.append(json.loads(line))\n",
    "        return puzzles\n",
    "\n",
    "    def _load_vocab(self, vocab_file: str) -> set:\n",
    "        vocab = set()\n",
    "        with open(vocab_file, 'r') as f:\n",
    "            for line in f:\n",
    "                word = line.strip().lower()\n",
    "                if len(word) == 5:\n",
    "                    vocab.add(word)\n",
    "        print(f\"Loaded vocabulary with {len(vocab)} words\")\n",
    "        return vocab\n",
    "\n",
    "    def _load_embeddings(self, embeddings_file: str) -> np.ndarray:\n",
    "        try:\n",
    "            embeddings = np.load(embeddings_file)\n",
    "            print(f\"Loaded embeddings: {embeddings.shape}\")\n",
    "            return embeddings.astype(np.float32)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Creating random embeddings\")\n",
    "            return np.random.randn(27, 8).astype(np.float32)\n",
    "\n",
    "    def _string_to_array(self, flat21: str) -> np.ndarray:\n",
    "        return np.array([ord(c) - ord('a') + 1 for c in flat21], dtype=np.int32)\n",
    "\n",
    "    def _array_to_string(self, arr: np.ndarray) -> str:\n",
    "        return \"\".join([chr(int(x) + ord('a') - 1) for x in arr])\n",
    "\n",
    "    def _get_word_for_position(self, pos: int) -> Optional[List[int]]:\n",
    "        for word in self.row_words:\n",
    "            if pos in word:\n",
    "                return word\n",
    "        for word in self.col_words:\n",
    "            if pos in word:\n",
    "                return word\n",
    "        return None\n",
    "\n",
    "    def _calculate_colors_and_embeddings(self, state: np.ndarray) -> np.ndarray:\n",
    "        features = np.zeros((21, 10), dtype=np.float32)\n",
    "        \n",
    "        for i in range(21):\n",
    "            current_letter = state[i]\n",
    "            target_letter = self.target_state[i]\n",
    "            \n",
    "            if current_letter == target_letter:\n",
    "                features[i, 0] = 1.0\n",
    "            else:\n",
    "                word_positions = self._get_word_for_position(i)\n",
    "                if word_positions:\n",
    "                    target_word_letters = [self.target_state[p] for p in word_positions]\n",
    "                    if current_letter in target_word_letters:\n",
    "                        features[i, 1] = 1.0\n",
    "            \n",
    "            letter_idx = int(current_letter) - 1 if current_letter >= 1 else 26\n",
    "            features[i, 2:10] = self.embeddings[letter_idx]\n",
    "        \n",
    "        return features\n",
    "\n",
    "    def _count_correct_positions(self, state: np.ndarray) -> int:\n",
    "        return np.sum(state == self.target_state)\n",
    "    \n",
    "    def _count_valid_words(self, state: np.ndarray) -> int:\n",
    "        current_letters = [chr(c + ord('a') - 1) for c in state]\n",
    "        valid_count = 0\n",
    "        \n",
    "        for indices in self.row_words + self.col_words:\n",
    "            word_str = ''.join([current_letters[i] for i in indices])\n",
    "            if word_str in self.vocab:\n",
    "                valid_count += 1\n",
    "        \n",
    "        return valid_count\n",
    "    \n",
    "    def _count_yellow_tiles(self, state: np.ndarray) -> int:\n",
    "        \"\"\"Count yellow letters\"\"\"\n",
    "        features = self._calculate_colors_and_embeddings(state)\n",
    "        return int(np.sum(features[:, 1]))  \n",
    "        \n",
    "    def reset(self, index: int) -> None:\n",
    "        self.current_puzzle = self.puzzles[index]\n",
    "        self.target_state = self._string_to_array(self.current_puzzle[\"target_flat21\"])\n",
    "        self.current_state = self._string_to_array(self.current_puzzle[\"shuffled_flat21\"])\n",
    "        self.fixed_indices = set(self.current_puzzle[\"fixed_indices\"])\n",
    "        self.moves = 0\n",
    "        self.max_moves = 50\n",
    "        self.prev_correct_count = self._count_correct_positions(self.current_state)\n",
    "        self.prev_word_count = self._count_valid_words(self.current_state)\n",
    "        self.prev_yellow_count = self._count_yellow_tiles(self.current_state)\n",
    "        self.prev_features = self._calculate_colors_and_embeddings(self.current_state)\n",
    "\n",
    "    def _update_state(self, action: int) -> None:\n",
    "        pos1 = action // 21\n",
    "        pos2 = action % 21\n",
    "        temp = self.current_state[pos1]\n",
    "        self.current_state[pos1] = self.current_state[pos2]\n",
    "        self.current_state[pos2] = temp\n",
    "        self.moves += 1\n",
    "\n",
    "    def _get_reward(self, action: int) -> float:\n",
    "        \"\"\"\n",
    "        Reward Shaping:Every improvement gets rewarded\n",
    "        \"\"\"\n",
    "        reward = -0.05  #step penalty\n",
    "        \n",
    "        # Final reward\n",
    "        if np.array_equal(self.current_state, self.target_state):\n",
    "            return 20.0 \n",
    "        \n",
    "        current_correct = self._count_correct_positions(self.current_state)\n",
    "        current_words = self._count_valid_words(self.current_state)\n",
    "        current_yellow = self._count_yellow_tiles(self.current_state)\n",
    "        \n",
    "        correct_change = current_correct - self.prev_correct_count\n",
    "        word_change = current_words - self.prev_word_count\n",
    "        yellow_change = current_yellow - self.prev_yellow_count\n",
    "        \n",
    "        # For Green\n",
    "        if correct_change > 0:\n",
    "            reward += 2.0 * correct_change\n",
    "        elif correct_change < 0:\n",
    "            reward += -1.0 * abs(correct_change)\n",
    "        \n",
    "        # Completing word\n",
    "        if word_change > 0:\n",
    "            reward += 3.0 * word_change\n",
    "        elif word_change < 0:\n",
    "            reward += -1.5 * abs(word_change)\n",
    "        \n",
    "        # Yellow tiles\n",
    "        if yellow_change > 0:\n",
    "            reward += 0.2 * yellow_change \n",
    "    \n",
    "        self.prev_correct_count = current_correct\n",
    "        self.prev_word_count = current_words\n",
    "        self.prev_yellow_count = current_yellow\n",
    "        self.prev_features = self._calculate_colors_and_embeddings(self.current_state)\n",
    "        \n",
    "        return np.clip(reward, -5.0, 20.0)\n",
    "\n",
    "    def _is_over(self) -> bool:\n",
    "        solved = np.array_equal(self.current_state, self.target_state)\n",
    "        max_moves_reached = self.moves >= self.max_moves\n",
    "        return solved or max_moves_reached\n",
    "    \n",
    "    def _is_win(self) -> bool:\n",
    "        return np.array_equal(self.current_state, self.target_state)\n",
    "\n",
    "    def observe(self) -> np.ndarray:\n",
    "        features = self._calculate_colors_and_embeddings(self.current_state)\n",
    "        grid = np.zeros((5, 5, 10), dtype=np.float32)\n",
    "\n",
    "        blank_embedding = self.embeddings[26]\n",
    "        blank_feature_vector = np.zeros(10, dtype=np.float32)\n",
    "        blank_feature_vector[2:10] = blank_embedding\n",
    "        blank_positions = [(1,1), (1,3), (3,1), (3,3)]\n",
    "        for r, c in blank_positions:\n",
    "            grid[r, c, :] = blank_feature_vector\n",
    "        \n",
    "        position_to_grid = [\n",
    "            (0,0), (0,1), (0,2), (0,3), (0,4),\n",
    "            (1,0), (1,2), (1,4),\n",
    "            (2,0), (2,1), (2,2), (2,3), (2,4),\n",
    "            (3,0), (3,2), (3,4),\n",
    "            (4,0), (4,1), (4,2), (4,3), (4,4)\n",
    "        ]\n",
    "        \n",
    "        for flat_idx in range(21):\n",
    "            r, c = position_to_grid[flat_idx]\n",
    "            grid[r, c, :] = features[flat_idx]\n",
    "        \n",
    "        return grid.reshape(1, 5, 5, 10)\n",
    "\n",
    "    def act(self, action: int) -> Tuple[np.ndarray, float, bool, bool]:\n",
    "        self._update_state(action)\n",
    "        reward = self._get_reward(action)\n",
    "        game_over = self._is_over()\n",
    "        game_win = self._is_win()\n",
    "        return self.observe(), reward, game_over, game_win\n",
    "\n",
    "    def get_valid_actions(self) -> List[int]:\n",
    "        valid_actions = []\n",
    "        for pos1 in range(21):\n",
    "            for pos2 in range(21):\n",
    "                if pos1 != pos2:\n",
    "                    action = pos1 * 21 + pos2\n",
    "                    valid_actions.append(action)\n",
    "        return valid_actions\n",
    "\n",
    "    def get_num_actions(self) -> int:\n",
    "        return 21 * 21\n",
    "\n",
    "    def get_current_state_letters(self) -> np.ndarray:\n",
    "        return self.current_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-22T20:17:15.645689Z",
     "iopub.status.busy": "2025-11-22T20:17:15.645478Z",
     "iopub.status.idle": "2025-11-22T20:17:15.651619Z",
     "shell.execute_reply": "2025-11-22T20:17:15.650737Z",
     "shell.execute_reply.started": "2025-11-22T20:17:15.645673Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing experience_replay.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile experience_replay.py\n",
    "\"\"\"\n",
    "Prioritized Experience Replay - Focuses on important experiences\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from typing import Tuple\n",
    "\n",
    "class PrioritizedExperienceReplay(object):\n",
    "    def __init__(self, max_memory: int = 20000, discount: float = 0.95, alpha: float = 0.6) -> None:\n",
    "        self.max_memory = max_memory\n",
    "        self.memory = list()\n",
    "        self.priorities = list()\n",
    "        self.discount = discount\n",
    "        self.alpha = alpha  \n",
    "\n",
    "    def add_experience(self, sars: list, game_over: bool, unmovable_positions = None, priority: float = 1.0) -> None:\n",
    "        self.memory.append([sars, game_over, unmovable_positions])\n",
    "        self.priorities.append(priority ** self.alpha)\n",
    "        \n",
    "        if len(self.memory) > self.max_memory:\n",
    "            del self.memory[0]\n",
    "            del self.priorities[0]\n",
    "\n",
    "    def get_qlearning_batch(\n",
    "        self, model: keras.Model, target_model: keras.Model, batch_size: int = 64\n",
    "    ) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        memory_length = len(self.memory)\n",
    "        num_inputs = min(memory_length, batch_size)\n",
    "        \n",
    "        probs = np.array(self.priorities) / sum(self.priorities)\n",
    "        ids = np.random.choice(memory_length, size=num_inputs, replace=False, p=probs)\n",
    "        \n",
    "        sars = list(zip(*[self.memory[id_][0] for id_ in ids]))\n",
    "        previous_states, action_ts, rewards, current_states = (\n",
    "            np.concatenate(e) if isinstance(e[0], np.ndarray) else np.stack(e)\n",
    "            for e in sars\n",
    "        )\n",
    "        game_over = np.array([self.memory[id_][1] for id_ in ids], dtype=bool)\n",
    "        \n",
    "        previous_states_tensor = tf.constant(previous_states, dtype=tf.float32)\n",
    "        current_states_tensor = tf.constant(current_states, dtype=tf.float32)\n",
    "        \n",
    "        targets = model(previous_states_tensor, training=False).numpy()\n",
    "        \n",
    "        next_q_main = model(current_states_tensor, training=False).numpy()\n",
    "        best_actions = np.argmax(next_q_main, axis=1)\n",
    "        \n",
    "        next_q_target = target_model(current_states_tensor, training=False).numpy()\n",
    "        Q_sa = next_q_target[np.arange(num_inputs), best_actions]\n",
    "        \n",
    "        td_errors = []\n",
    "        for i in range(num_inputs):\n",
    "            old_q = targets[i, action_ts[i]]\n",
    "            new_q = rewards[i] + self.discount * Q_sa[i] * (not game_over[i])\n",
    "            td_error = abs(new_q - old_q)\n",
    "            td_errors.append(td_error)\n",
    "            targets[i, action_ts[i]] = new_q\n",
    "        \n",
    "        for i, idx in enumerate(ids):\n",
    "            self.priorities[idx] = (td_errors[i] + 1e-5) ** self.alpha\n",
    "        \n",
    "        return previous_states, targets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-22T20:17:15.653272Z",
     "iopub.status.busy": "2025-11-22T20:17:15.653032Z",
     "iopub.status.idle": "2025-11-22T20:17:36.673586Z",
     "shell.execute_reply": "2025-11-22T20:17:36.672647Z",
     "shell.execute_reply.started": "2025-11-22T20:17:15.653257Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-22 20:17:17.281692: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1763842637.508792      48 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1763842637.578951      48 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.18.0\n",
      "GPUs available: 2\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPUs available: {len(tf.config.list_physical_devices('GPU'))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-22T20:18:55.197566Z",
     "iopub.status.busy": "2025-11-22T20:18:55.196747Z",
     "iopub.status.idle": "2025-11-22T20:18:55.203437Z",
     "shell.execute_reply": "2025-11-22T20:18:55.202614Z",
     "shell.execute_reply.started": "2025-11-22T20:18:55.197539Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting model_definition.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile model_definition.py\n",
    "\"\"\"\n",
    "Simplified stable model - NO CLIPPING VERSION\n",
    "\"\"\"\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "def define_model(input_shape, hidden_size, num_actions, learning_rate, conv_filters=32):\n",
    "    \"\"\"Simpler architecture - rely on gradient clipping only\"\"\"\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "\n",
    "    x = layers.Conv2D(conv_filters, (3, 3), activation='relu', padding='same')(inputs)\n",
    "    x = layers.Conv2D(conv_filters, (3, 3), activation='relu', padding='same')(x)\n",
    "    \n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(hidden_size, activation='relu')(x)\n",
    "    x = layers.Dense(hidden_size // 2, activation='relu')(x)\n",
    "    \n",
    "    outputs = layers.Dense(num_actions, activation='linear')(x)\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    optimizer = keras.optimizers.Adam(learning_rate=learning_rate, clipnorm=0.5)\n",
    "    model.compile(optimizer=optimizer, loss='mse')\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-22T20:18:58.058614Z",
     "iopub.status.busy": "2025-11-22T20:18:58.058283Z",
     "iopub.status.idle": "2025-11-22T20:36:54.965170Z",
     "shell.execute_reply": "2025-11-22T20:36:54.964497Z",
     "shell.execute_reply.started": "2025-11-22T20:18:58.058593Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Starting GUIDED EXPLORATION training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.18.0\n",
      "GPUs available: 2\n",
      "\n",
      "======================================================================\n",
      "HYPERPARAMETERS - GUIDED EXPLORATION\n",
      "======================================================================\n",
      "Epochs: 250 | LR: 0.0005 | Discount: 0.95\n",
      "Epsilon: 0.95→0.3 | Hidden: 64\n",
      "Exploration: 50% random + 50% greedy lookahead\n",
      "Prioritized replay: HIGH reward experiences trained more\n",
      "======================================================================\n",
      "\n",
      "Loaded embeddings: (27, 8)\n",
      "Loaded vocabulary with 5757 words\n",
      "✓ Environment loaded (2000 puzzles)\n",
      "\n",
      "Building models...\n",
      "✓ Models built\n",
      "\n",
      "======================================================================\n",
      "STARTING GUIDED EXPLORATION TRAINING\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "GUIDED EXPLORATION + ULTRA-DENSE REWARDS\n",
      "======================================================================\n",
      "Very Easy (≥12): 5 puzzles\n",
      "Easy (9-11):     260 puzzles\n",
      "Medium (7-8):    964 puzzles\n",
      "Hard (<7):       771 puzzles\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1763842738.593444      92 cuda_dnn.cc:529] Loaded cuDNN version 90300\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1763842740.810835      48 service.cc:148] XLA service 0x21419810 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1763842740.811948      48 service.cc:156]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
      "I0000 00:00:1763842740.811973      48 service.cc:156]   StreamExecutor device (1): Tesla T4, Compute Capability 7.5\n",
      "I0000 00:00:1763842742.852549      48 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ★ WIN! Puzzle 1971 in 39 moves\n",
      "  ★ WIN! Puzzle 1867 in 37 moves\n",
      "Ep    1/250 | Loss: 19012.4 | ε: 0.945 | Wins:   2 ( 2.0%) | R:  42.6 | Correct: 12.9/21 | Words: 1.3/6\n",
      "  ★ WIN! Puzzle 606 in 44 moves\n",
      "  ★ WIN! Puzzle 1080 in 46 moves\n",
      "  ★ WIN! Puzzle 494 in 45 moves\n",
      "  ★ WIN! Puzzle 973 in 27 moves\n",
      "  ★ WIN! Puzzle 1213 in 22 moves\n",
      "Ep    2/250 | Loss: 5287.0 | ε: 0.939 | Wins:   7 ( 3.5%) | R:  42.7 | Correct: 12.9/21 | Words: 1.3/6\n",
      "  ★ WIN! Puzzle 1 in 47 moves\n",
      "  ★ WIN! Puzzle 296 in 12 moves\n",
      "  ★ WIN! Puzzle 1511 in 50 moves\n",
      "  ★ WIN! Puzzle 887 in 38 moves\n",
      "  ★ WIN! Puzzle 1511 in 34 moves\n",
      "  ★ WIN! Puzzle 1252 in 47 moves\n",
      "Ep    3/250 | Loss: 3260.9 | ε: 0.934 | Wins:  13 ( 4.3%) | R:  42.4 | Correct: 12.6/21 | Words: 1.2/6\n",
      "  ★ WIN! Puzzle 1018 in 12 moves\n",
      "  ★ WIN! Puzzle 1262 in 47 moves\n",
      "  ★ WIN! Puzzle 954 in 36 moves\n",
      "  ★ WIN! Puzzle 121 in 37 moves\n",
      "Ep    4/250 | Loss: 2522.6 | ε: 0.928 | Wins:  17 ( 4.2%) | R:  42.5 | Correct: 12.8/21 | Words: 1.2/6\n",
      "  ★ WIN! Puzzle 793 in 21 moves\n",
      "  ★ WIN! Puzzle 1455 in 47 moves\n",
      "Ep    5/250 | Loss: 2355.8 | ε: 0.923 | Wins:  19 ( 3.8%) | R:  42.0 | Correct: 12.4/21 | Words: 1.1/6\n",
      "  ★ WIN! Puzzle 142 in 31 moves\n",
      "  ★ WIN! Puzzle 1088 in 22 moves\n",
      "Ep    6/250 | Loss: 2784.6 | ε: 0.918 | Wins:  21 ( 3.5%) | R:  41.7 | Correct: 13.1/21 | Words: 1.1/6\n",
      "  ★ WIN! Puzzle 133 in 35 moves\n",
      "  ★ WIN! Puzzle 890 in 29 moves\n",
      "Ep    7/250 | Loss: 3871.4 | ε: 0.912 | Wins:  23 ( 3.3%) | R:  42.2 | Correct: 12.6/21 | Words: 1.2/6\n",
      "Ep    8/250 | Loss: 5944.5 | ε: 0.907 | Wins:  23 ( 2.9%) | R:  40.8 | Correct: 12.4/21 | Words: 1.1/6\n",
      "  ★ WIN! Puzzle 1071 in 30 moves\n",
      "  ★ WIN! Puzzle 908 in 34 moves\n",
      "  ★ WIN! Puzzle 52 in 24 moves\n",
      "Ep    9/250 | Loss: 9572.7 | ε: 0.902 | Wins:  26 ( 2.9%) | R:  41.0 | Correct: 12.7/21 | Words: 1.1/6\n",
      "  ★ WIN! Puzzle 1920 in 26 moves\n",
      "  ★ WIN! Puzzle 1435 in 39 moves\n",
      "  ★ WIN! Puzzle 615 in 19 moves\n",
      "Ep   10/250 | Loss: 19112.8 | ε: 0.897 | Wins:  29 ( 2.9%) | R:  40.8 | Correct: 12.7/21 | Words: 1.1/6\n",
      "  ★ WIN! Puzzle 1419 in 17 moves\n",
      "Ep   15/250 | Loss: 810312.0 | ε: 0.871 | Wins:  30 ( 2.0%) | R:  40.3 | Correct: 12.3/21 | Words: 0.9/6\n",
      "  ★ WIN! Puzzle 512 in 28 moves\n",
      "  ★ WIN! Puzzle 822 in 26 moves\n",
      "  ⚠️  Training unstable, stopping\n",
      "\n",
      "✓ Saved to /kaggle/working/model/\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "GUIDED EXPLORATION DQN - Helps agent discover good actions\n",
    "\"\"\"\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPUs available: {len(tf.config.list_physical_devices('GPU'))}\")\n",
    "\n",
    "dataset_path = \"/kaggle/input/waffle/waffles_shuffled_2k.jsonl\"\n",
    "embeddings_path = \"/kaggle/input/waffle/letter_embeddings.npy\"\n",
    "vocab_path = \"/kaggle/input/waffle/words.txt\"\n",
    "\n",
    "for mod in ['env', 'experience_replay', 'model_definition']:\n",
    "    if mod in sys.modules:\n",
    "        del sys.modules[mod]\n",
    "\n",
    "from env import Waffle\n",
    "from experience_replay import PrioritizedExperienceReplay\n",
    "from model_definition import define_model\n",
    "\n",
    "def guided_action_selection(env, epsilon, q_values, valid_actions, num_actions):\n",
    "    \"\"\"\n",
    "    GUIDED EXPLORATION: Mix random, greedy-lookahead, and Q-learning\n",
    "    \"\"\"\n",
    "    if np.random.rand() < epsilon:\n",
    "        \n",
    "        if np.random.rand() < 0.5:\n",
    "            return np.random.choice(valid_actions)\n",
    "        else:\n",
    "\n",
    "            best_action = None\n",
    "            best_score = -999\n",
    "            \n",
    "            movable = [p for p in range(21) if p not in env.fixed_indices]\n",
    "            sample_actions = np.random.choice(len(movable) * (len(movable) - 1), \n",
    "                                             size=min(20, len(valid_actions)), \n",
    "                                             replace=False)\n",
    "            \n",
    "            for _ in range(min(20, len(valid_actions))):\n",
    "                action = np.random.choice(valid_actions)\n",
    "                pos1, pos2 = action // 21, action % 21\n",
    "       \n",
    "                env.current_state[pos1], env.current_state[pos2] = \\\n",
    "                    env.current_state[pos2], env.current_state[pos1]\n",
    "                \n",
    "                score = (env._count_correct_positions(env.current_state) * 2 +\n",
    "                        env._count_valid_words(env.current_state) * 3)\n",
    "          \n",
    "                env.current_state[pos1], env.current_state[pos2] = \\\n",
    "                    env.current_state[pos2], env.current_state[pos1]\n",
    "                \n",
    "                if score > best_score:\n",
    "                    best_score = score\n",
    "                    best_action = action\n",
    "            \n",
    "            return best_action if best_action else np.random.choice(valid_actions)\n",
    "    else:\n",
    "        masked_q = np.full(num_actions, -np.inf)\n",
    "        masked_q[valid_actions] = q_values[valid_actions]\n",
    "        return int(np.argmax(masked_q))\n",
    "\n",
    "def train_model(model, target_model, env, epochs, experience_replay, \n",
    "               epsilon_start, epsilon_end, epsilon_decay, batch_size, \n",
    "               puzzles_per_epoch, target_update_freq):\n",
    "    logging.info(\"Starting GUIDED EXPLORATION training...\")\n",
    "    win_count = 0\n",
    "    total_rewards = []\n",
    "    epsilon = epsilon_start\n",
    "    num_actions = env.get_num_actions()\n",
    "    training_steps = 0\n",
    "    initial_correctness_list = []\n",
    "    for idx, p in enumerate(env.puzzles):\n",
    "        state = env._string_to_array(p[\"shuffled_flat21\"])\n",
    "        target = env._string_to_array(p[\"target_flat21\"])\n",
    "        correct = np.sum(state == target)\n",
    "        initial_correctness_list.append((idx, correct))\n",
    "\n",
    "    bins = {\n",
    "        'veryeasy': [idx for idx, c in initial_correctness_list if c >= 12], \n",
    "        'easy':     [idx for idx, c in initial_correctness_list if 9 <= c < 12],\n",
    "        'medium':   [idx for idx, c in initial_correctness_list if 7 <= c < 9],\n",
    "        'hard':     [idx for idx, c in initial_correctness_list if c < 7]\n",
    "    }\n",
    "\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"GUIDED EXPLORATION + ULTRA-DENSE REWARDS\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Very Easy (≥12): {len(bins['veryeasy'])} puzzles\")\n",
    "    print(f\"Easy (9-11):     {len(bins['easy'])} puzzles\")\n",
    "    print(f\"Medium (7-8):    {len(bins['medium'])} puzzles\")\n",
    "    print(f\"Hard (<7):       {len(bins['hard'])} puzzles\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "\n",
    "    def sample_puzzles(epoch):\n",
    "        fraction = epoch / epochs\n",
    "        if fraction < 0.60:\n",
    "            return np.random.choice(bins['veryeasy'] + bins['easy'], \n",
    "                                   size=puzzles_per_epoch, replace=True)\n",
    "        elif fraction < 0.85:\n",
    "            return np.random.choice(bins['easy'] + bins['medium'], \n",
    "                                   size=puzzles_per_epoch, replace=True)\n",
    "        else:\n",
    "            all_puzzles = bins['easy'] + bins['medium'] + bins['hard']\n",
    "            return np.random.choice(all_puzzles, size=puzzles_per_epoch, replace=True)\n",
    "    \n",
    "    @tf.function\n",
    "    def predict_q(state):\n",
    "        return model(state, training=False)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        loss = 0.0\n",
    "        final_correct = []\n",
    "        final_words = []\n",
    "        epoch_wins = 0\n",
    "        \n",
    "        puzzle_indices = sample_puzzles(epoch)\n",
    "        \n",
    "        for puzzle_idx in puzzle_indices:\n",
    "            env.reset(puzzle_idx)\n",
    "            \n",
    "            if np.array_equal(env.current_state, env.target_state):\n",
    "                continue\n",
    "            \n",
    "            current_state = env.observe()\n",
    "            game_over = False\n",
    "            episode_reward = 0\n",
    "            step_count = 0\n",
    "            \n",
    "            fixed_positions = env.fixed_indices\n",
    "            patience = 60 \n",
    "            no_progress_steps = 0\n",
    "            best_correct = env._count_correct_positions(env.current_state)\n",
    "            \n",
    "            while not game_over:\n",
    "                current_correct = env._count_correct_positions(env.current_state)\n",
    "                if current_correct > best_correct:\n",
    "                    no_progress_steps = 0\n",
    "                    best_correct = current_correct\n",
    "                else:\n",
    "                    no_progress_steps += 1\n",
    "\n",
    "                if no_progress_steps >= patience:\n",
    "                    game_over = True\n",
    "                    break\n",
    "                \n",
    "                previous_state = current_state\n",
    "                \n",
    "                movable = [p for p in range(21) if p not in fixed_positions]\n",
    "                valid_actions = []\n",
    "                for pos1 in movable:\n",
    "                    for pos2 in movable:\n",
    "                        if pos1 != pos2:\n",
    "                            action = pos1 * 21 + pos2\n",
    "                            valid_actions.append(action)\n",
    "             \n",
    "                q = predict_q(tf.constant(previous_state, dtype=tf.float32)).numpy()[0]\n",
    "                action = guided_action_selection(env, epsilon, q, valid_actions, num_actions)\n",
    "\n",
    "                current_state, reward, game_over, game_win = env.act(action)\n",
    "                episode_reward += reward\n",
    "                step_count += 1\n",
    "               \n",
    "                priority = max(1.0, abs(reward))\n",
    "                experience_replay.add_experience(\n",
    "                    [previous_state, int(action), reward, current_state], \n",
    "                    game_over,\n",
    "                    fixed_positions,\n",
    "                    priority=priority\n",
    "                )\n",
    "                \n",
    "                if len(experience_replay.memory) >= batch_size and step_count % 3 == 0:\n",
    "                    inputs, targets = experience_replay.get_qlearning_batch(\n",
    "                        model, target_model, batch_size=batch_size\n",
    "                    )\n",
    "                    batch_loss = model.train_on_batch(inputs, targets)\n",
    "                    \n",
    "                    if np.isnan(batch_loss) or batch_loss > 5000:\n",
    "                        print(f\"  ⚠️  Training unstable, stopping\")\n",
    "                        return model\n",
    "                    \n",
    "                    loss += batch_loss\n",
    "                    training_steps += 1\n",
    "                    \n",
    "                    if training_steps % target_update_freq == 0:\n",
    "                        target_model.set_weights(model.get_weights())\n",
    "            \n",
    "            total_rewards.append(episode_reward)\n",
    "            final_correct.append(env._count_correct_positions(env.current_state))\n",
    "            final_words.append(env._count_valid_words(env.current_state))\n",
    "            \n",
    "            if game_win:\n",
    "                epoch_wins += 1\n",
    "                print(f\"  ★ WIN! Puzzle {puzzle_idx} in {step_count} moves\")\n",
    "        \n",
    "        win_count += epoch_wins\n",
    "        epsilon = max(epsilon_end, epsilon * epsilon_decay)\n",
    "        \n",
    "        if (epoch + 1) % 5 == 0 or epoch < 10:\n",
    "            recent = min(150, len(total_rewards))\n",
    "            avg_reward = np.mean(total_rewards[-recent:])\n",
    "            win_rate = win_count / ((epoch + 1) * puzzles_per_epoch) * 100\n",
    "            avg_correct = np.mean(final_correct[-recent:])\n",
    "            avg_words = np.mean(final_words[-recent:])\n",
    "            \n",
    "            sample_state = env.observe()\n",
    "            q_values = model(tf.constant(sample_state, dtype=tf.float32), training=False).numpy()[0]\n",
    "            \n",
    "            print(f\"Ep {epoch+1:4d}/{epochs} | Loss: {loss:6.1f} | ε: {epsilon:.3f} | \"\n",
    "                  f\"Wins: {win_count:3d} ({win_rate:4.1f}%) | R: {avg_reward:5.1f} | \"\n",
    "                  f\"Correct: {avg_correct:.1f}/21 | Words: {avg_words:.1f}/6\")\n",
    "        \n",
    "        if (epoch + 1) % 50 == 0:\n",
    "            checkpoint_dir = '/kaggle/working/model_checkpoints'\n",
    "            os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "            model.save_weights(f'{checkpoint_dir}/model_epoch_{epoch+1}.weights.h5')\n",
    "            print(f\"  ✓ Saved checkpoint\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "EPOCHS = 250 \n",
    "EPSILON_START = 0.95 \n",
    "EPSILON_END = 0.3  \n",
    "TARGET_EPSILON_EPOCH = 200\n",
    "EPSILON_DECAY = (EPSILON_END / EPSILON_START) ** (1 / TARGET_EPSILON_EPOCH)\n",
    "MAX_MEMORY = 20000\n",
    "HIDDEN_SIZE = 64 \n",
    "CONV_FILTERS = 16\n",
    "BATCH_SIZE = 32\n",
    "DISCOUNT = 0.95\n",
    "LEARNING_RATE = 0.0005\n",
    "PUZZLES_PER_EPOCH = 100\n",
    "TARGET_UPDATE_FREQUENCY = 500\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"HYPERPARAMETERS - GUIDED EXPLORATION\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Epochs: {EPOCHS} | LR: {LEARNING_RATE} | Discount: {DISCOUNT}\")\n",
    "print(f\"Epsilon: {EPSILON_START}→{EPSILON_END} | Hidden: {HIDDEN_SIZE}\")\n",
    "print(f\"Exploration: 50% random + 50% greedy lookahead\")\n",
    "print(f\"Prioritized replay: HIGH reward experiences trained more\")\n",
    "print(f\"{'='*70}\\n\")\n",
    "\n",
    "env = Waffle(puzzles_file=dataset_path, embeddings_file=embeddings_path, vocab_file=vocab_path)\n",
    "num_actions = env.get_num_actions()\n",
    "input_shape = (5, 5, 10)\n",
    "\n",
    "print(f\"✓ Environment loaded ({len(env.puzzles)} puzzles)\\n\")\n",
    "\n",
    "print(\"Building models...\")\n",
    "model = define_model(input_shape, HIDDEN_SIZE, num_actions, LEARNING_RATE, CONV_FILTERS)\n",
    "target_model = define_model(input_shape, HIDDEN_SIZE, num_actions, LEARNING_RATE, CONV_FILTERS)\n",
    "target_model.set_weights(model.get_weights())\n",
    "print(\"✓ Models built\\n\")\n",
    "\n",
    "exp_replay = PrioritizedExperienceReplay(max_memory=MAX_MEMORY, discount=DISCOUNT)\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"STARTING GUIDED EXPLORATION TRAINING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "trained_model = train_model(\n",
    "    model, target_model, env, EPOCHS, exp_replay,\n",
    "    EPSILON_START, EPSILON_END, EPSILON_DECAY,\n",
    "    BATCH_SIZE, PUZZLES_PER_EPOCH, TARGET_UPDATE_FREQUENCY\n",
    ")\n",
    "\n",
    "output_dir = '/kaggle/working/model'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "trained_model.save_weights(f'{output_dir}/model.weights.h5', overwrite=True)\n",
    "print(f\"\\n✓ Saved to {output_dir}/\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 14591356,
     "datasetId": 8744658,
     "sourceId": 13832119,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
